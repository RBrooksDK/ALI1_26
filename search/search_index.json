{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> Applied Linear Algebra - 2025 <p>Repository for ALI1-S25 at VIA</p> <p>Checkout the homepage!</p> </p> <p>        Your browser does not support the video tag.      </p>"},{"location":"#course-information","title":"Course information","text":"<ul> <li>Course responsible: Associate Professor Richard Brooks, rib@via.dk</li> <li>5 ECTS (European Credit Transfer System), corresponding to 130 hours of work</li> <li>12 sessions, each with a duration of 4 lessons</li> <li>Bachelor level course</li> <li>Grade: 7-step scale</li> <li>Type of assessment: 4-hour written exam (see exam description in the menu to the left)</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>It is important that you recap some of your high-school math. Most importantly:</p> <ul> <li>Linear equations</li> <li>Systems of linear equations</li> <li>Vectors and vector operations</li> <li>Differential equations</li> </ul>"},{"location":"#lectures-and-course-organization","title":"Lectures and course organization","text":"<p>The course is scheduled to start Friday February 6 and will continue up until and including May 1. All sessions are from 8:20 to 11:50 in room C05.15. In general, each session is made up of four activities:</p> <ol> <li>At the beginning of each session, there will be a short recap of the previous session.</li> <li>We then go through the exercises from the previous session.</li> <li>We will go through the theory of the current session.</li> <li>After classes, and before the next session, you will have to solve exercises from the current session.</li> </ol> <p>This then loops back to (1) at the beginning of the next session.</p> <p>There are no mandatory assignments, but it is highly recommended to work on the exercises for each session. No instruction is provided for the exercises so you will have to work on them on your own or form study groups.</p>"},{"location":"#course-content-and-learning-objectives","title":"Course content and learning objectives","text":"<p>Applied Linear Algebra focuses on understanding and applying the core concepts of linear algebra to solve real-world problems. The course explores vector spaces, matrices, eigenvalues, and eigenvectors, emphasising their practical applications in fields such as computer graphics, machine learning, and engineering. The course is designed to provide students with a solid foundation in linear algebra, enabling them to tackle complex problems and develop analytical skills.</p> <p>Learning Objectives</p> <ul> <li>Linear Systems: Understand the concept of a linear system of equations, how to represent them with matrices, and how to solve them using row reduction.</li> <li>Matrix Algebra: Perform matrix operations including addition, multiplication, and inversion. Understand matrices as transformations and systems of linear equations, and learn to use matrices for practical problem solving.</li> <li>Determinants and Invertibility: Compute determinants of matrices and understand their geometric and algebraic significance. Use determinants to assess matrix invertibility and to solve linear systems.</li> <li>Vectors and Vector Spaces: Understand the fundamental concepts of vectors, vector operations, and vector spaces. Learn to interpret vectors algebraically and geometrically, and reason about spans, bases, dimensions, and linear independence.</li> <li>Markov Chains: Understand the concept of a Markov chain, how to represent them with matrices, and how to solve them using row reduction.</li> <li>Linear Transformations and Eigenanalysis: Understand linear transformations through matrix multiplication. Learn to find eigenvalues and eigenvectors, diagonalise matrices when possible, and interpret eigenanalysis in practical contexts.</li> <li>Systems of Differential Equations: Apply linear algebra techniques to solve systems of first-order differential equations. Understand the connection between eigenvalues, eigenvectors, and the behaviour of dynamic systems.</li> <li>Orthogonality and Least Squares: Explore orthogonality in vector spaces, apply the Gram-Schmidt process, and solve least squares problems. Understand projections and their role in approximating inconsistent systems and fitting models to data.</li> <li>Symmetric Matrices, SVD, and PCA: Analyse symmetric matrices, perform Singular Value Decomposition (SVD), and understand its application to data reduction and Principal Component Analysis (PCA).</li> </ul>"},{"location":"#resources","title":"Resources","text":"<p>Lay: Lay, David C. Linear Algebra and its applications, 4<sup>th</sup> edition. (e-book, up to students to retrieve a copy). All chapters and exercises referenced will be to the 4<sup>th</sup> edition. Make sure you have the correct edition or else the exercise numbers will not match.</p> <p>Note, for each lesson I have uploaded the presentations that accompany the book. I will in no way use these during classes and they are merely uploaded for your notes/convenience. The notes that I have uploaded are an electronic version of my personal lecture notes and contain most of the (relevant) material for the sections in questions. Since some of the exercises are from the book, you may see [M] associated with exercises. This means that you are supposed to solve the exercise using Matlab. Disregard this and use Python.</p> <p>Non-session specific resources such as the exercises from the book, solutions, old exam cases, etc. can be found her:</p> <p>General Resources ALI</p> <p>This folder is always accessible in the menu to the left.</p> <p>The Wiseflow code for all flows that are used during the course is always 0000. This is not the code for the actual exam in June, though.</p> <p>Suggested online resources can be found in the menu to the left. These are not mandatory, but they can be useful for some students.</p> <p>Make sure you install a working version of Jupyter Notebook and Python version 3.7 or higher. You can choose whichever IDE you want to work in as long as it can handle Jupyter Notebooks. Installing VS Code with a Jupyter Notebook extension seems to be a popular choice.</p>"},{"location":"#historical-notes","title":"Historical Notes","text":"<p>Applied Linear Algebra was first offered in 2014 and is scheduled to be taught 1\u20132 times per year. The course responsible is Richard Brooks (RIB) who has been the only lecturer teaching the course.</p> Grade Distribution 2025 (includes re-exams from 2023 and 2024) Grade Count 128 105 73 44 022 008 -30"},{"location":"01_Introduction_to_Linear_Algebra/","title":"01 Introduction to Linear Algebra","text":"Introduction to Linear Algebra"},{"location":"01_Introduction_to_Linear_Algebra/#session-material","title":"Session Material:","text":"<p>Lay: 1.1-1.5  + 1.7-1.10 (1.8-1.10 self-study)  </p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"01_Introduction_to_Linear_Algebra/#session-description","title":"Session Description","text":"<p>We'll begin with systems of linear equations, what they are, how to represent them with matrices, and how to solve them using row reduction. You'll learn key row operations, how to reach echelon forms, identify pivots, and determine basic and free variables. We'll cover when systems have unique, infinite, or no solutions, revisiting concepts like Theorem 2.</p> <p>Next, we introduce vectors: linear combinations, spans, and how vector equations relate to linear systems, with geometric intuition in 2D and 3D. We'll examine linear dependence and independence, identifying redundant vectors and linking this to simple homogeneous systems (Theorems 7\u20139). Finally, we'll introduce linear transformations, how matrices map vectors between spaces, laying a vital foundation for what's to come.</p>"},{"location":"01_Introduction_to_Linear_Algebra/#key-concepts","title":"Key Concepts","text":"<ul> <li>Linear Systems</li> <li>Matrices</li> <li>Row Reduction</li> <li>Vectors</li> <li>Span</li> <li>Linear Dependence</li> <li>Linear Transformations</li> </ul> <p>Learning Objectives</p> <ul> <li>Identify and represent linear systems using matrices.</li> <li>Apply row reduction techniques to solve linear systems.</li> <li>Describe and construct vectors, spans, and linear combinations.</li> <li>Distinguish and determine linear dependence and independence in sets of vectors.</li> <li>Analyze and interpret linear transformations and their relationship with matrices.</li> </ul>"},{"location":"01_Introduction_to_Linear_Algebra/#quiz","title":"Quiz","text":"<p>After the session, you should be able to answer this quiz. You can see your score at the bottom of the page. Your answers are not saved or recorded.</p> Question 1A system of linear equations has no solution if its augmented matrix, after row reduction to echelon form, contains a row of the form:$[0 \\; 0 \\; \\dots \\; 0 \\; | \\; 0]$A row where every entry is non-zero.$[1 \\; 0 \\; \\dots \\; 0 \\; | \\; b]$ where $b$ is any real number.$[0 \\; 0 \\; \\dots \\; 0 \\; | \\; b]$ where $b$ is a non-zero real number.A column corresponding to a free variable.Check AnswerScore: 0 / 1<p>Explanation: A row of the form $[0 \\; 0 \\; \\dots \\; 0 \\; | \\; b]$ where $b \\neq 0$ represents the equation $0x_1 + 0x_2 + \\dots + 0x_n = b$. If $b$ is non-zero, this equation is $0 = b$, which is a contradiction. Therefore, the system is inconsistent and has no solution. This relates to understanding when systems have unique, infinite, or no solutions based on their echelon form.</p> Question 2Consider the matrix equation $Ax = b$. If the vector $b$ can be written as a linear combination of the columns of matrix $A$, what can be said about the system?The system $Ax = b$ is consistent (has at least one solution).The system has no solution.The columns of $A$ are linearly dependent.The matrix $A$ must be invertible.The system has a unique solution only if $A$ is an identity matrix.Check AnswerScore: 0 / 1<p>Explanation: The definition of $Ax$ is a linear combination of the columns of $A$ using the entries of $x$ as weights. If $b$ is in the span of the columns of $A$ (meaning $b$ is a linear combination of the columns of $A$), then there exists at least one vector $x$ such that $Ax = b$. This means the system is consistent.</p> Question 3Which of the following sets of vectors in $\\mathbb{R}^3$ is linearly dependent. Let $v_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $v_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $v_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$, $v_4 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 0 \\end{bmatrix}.$$\\{v_1, v_2, v_3\\}$$\\{v_1, v_2\\}$$\\{v_2, v_3\\}$$\\{v_1, v_3\\}$$\\{v_1, v_4\\}$Check AnswerScore: 0 / 1<p>Explanation: A set of vectors is linearly dependent if at least one vector in the set can be written as a linear combination of the others, or equivalently, if there's a non-trivial solution to $c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\dots + c_p\\mathbf{v}_p = \\mathbf{0}$. In this case, $v_4 = 2v_1$. Thus, $2v_1 - v_4 = \\mathbf{0}$ is a non-trivial linear combination that equals the zero vector, making the set $\\{v_1, v_4\\}$ linearly dependent. The set $\\{v_1, v_2, v_3\\}$ is the standard basis for $\\mathbb{R}^3$ and is linearly independent.</p> Question 4A linear transformation $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ is defined by $T(x) = Ax$. If $A$ is an $m \\times n$ matrix with $n$ pivot positions (i.e., a pivot in every column), what can be concluded about the columns of $A$?The matrix $A$ has $m$ rows.The columns of $A$ are linearly independent.The columns of $A$ span $\\mathbb{R}^m$.The transformation $T$ is onto $\\mathbb{R}^m$.The equation $Ax = \\mathbf{0}$ has infinitely many solutions.Check AnswerScore: 0 / 1<p>Explanation: If an $m \\times n$ matrix $A$ has $n$ pivot positions, it means there is a pivot in every column. This implies that the equation $Ax = \\mathbf{0}$ has only the trivial solution. By definition (Theorem 7, or related concepts), this means the columns of $A$ are linearly independent. It does not guarantee that $T$ is onto unless $m=n$, nor does it guarantee that the columns span $\\mathbb{R}^m$ unless $m \\le n$ and there are $m$ pivots.</p> Question 5Given the augmented matrix $\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}$ which is in echelon form, how many solutions does the corresponding system of linear equations have?Exactly two solutions.No solution.Infinitely many solutions.Exactly one unique solution.The number of solutions cannot be determined from this form.Check AnswerScore: 0 / 1<p>Explanation: The matrix represents the system $x_1 + 2x_2 = 3$ and $0x_1 + 0x_2 = 0$. The second equation $0=0$ is always true and provides no restriction. The first column (for $x_1$) has a pivot, but the second column (for $x_2$) does not. This means $x_2$ is a free variable. Since there is at least one free variable and the system is consistent (no row like $[0 \\; 0 \\; | \\; b]$ where $b \\neq 0$), the system has infinitely many solutions. This relates to identifying pivots and basic/free variables.</p> Question 6Consider the system of linear equations: $x_1 + 2x_2 - x_3 = 1$, $2x_1 + 5x_2 + x_3 = 4$, $x_1 + 3x_2 + 2x_3 = 3$. After performing row operations to bring the augmented matrix to row echelon form, one possible row echelon form is $\\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; | &amp; 1 \\\\ 0 &amp; 1 &amp; 3 &amp; | &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; | &amp; 0 \\end{bmatrix}$. Which variable is a free variable?$x_1$There are no free variables.$x_3$$x_2$Both $x_2$ and $x_3$ are free variables.Check AnswerScore: 0 / 1<p>Explanation: In the given row echelon form, the pivot columns are the first and second columns (corresponding to $x_1$ and $x_2$). The third column (corresponding to $x_3$) does not contain a pivot. Therefore, $x_3$ is a free variable. The system has infinitely many solutions.</p> Question 7Let $A = \\begin{bmatrix} 1 &amp; -2 &amp; 1 \\\\ 0 &amp; 2 &amp; -8 \\\\ -4 &amp; 5 &amp; 9 \\end{bmatrix}$ and $b = \\begin{bmatrix} 0 \\\\ 8 \\\\ -9 \\end{bmatrix}$. The augmented matrix $[A|b]$ can be row reduced to $\\begin{bmatrix} 1 &amp; 0 &amp; -7 &amp; | &amp; 8 \\\\ 0 &amp; 1 &amp; -4 &amp; | &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; | &amp; 0 \\end{bmatrix}$. What is the solution $x$ to $Ax = b$ if we let the free variable be $x_3 = t$?$x = \\begin{bmatrix} 8 \\\\ 4 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -7 \\\\ -4 \\\\ 0 \\end{bmatrix}$$x = \\begin{bmatrix} 8 \\\\ 4 \\\\ t \\end{bmatrix}$$x = \\begin{bmatrix} -7 \\\\ -4 \\\\ 1 \\end{bmatrix} + t \\begin{bmatrix} 8 \\\\ 4 \\\\ 0 \\end{bmatrix}$The system has no solution.$x = \\begin{bmatrix} 8 \\\\ 4 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 7 \\\\ 4 \\\\ 1 \\end{bmatrix}$Check AnswerScore: 0 / 1<p>Explanation: From the reduced row echelon form, we have the equations: $x_1 - 7x_3 = 8$ and $x_2 - 4x_3 = 4$. Since $x_3$ is free, let $x_3 = t$. Then $x_1 = 8 + 7t$ and $x_2 = 4 + 4t$. In vector form, this is $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 8 + 7t \\\\ 4 + 4t \\\\ t \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 4 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} 7 \\\\ 4 \\\\ 1 \\end{bmatrix}$.</p> Question 8For what value(s) of $h$ will the vectors $v_1 = \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix}$, $v_2 = \\begin{bmatrix} 5 \\\\ -4 \\\\ -7 \\end{bmatrix}$, $v_3 = \\begin{bmatrix} -3 \\\\ 1 \\\\ h \\end{bmatrix}$ be linearly dependent?$h = 0$ onlyFor all values of $h$.For no values of $h$.$h = -1$$h = 3$Check AnswerScore: 0 / 1<p>Explanation: The vectors are linearly dependent if the matrix $A = [v_1 \\ v_2 \\ v_3]$ has fewer than 3 pivots, or if the augmented matrix $[A | \\mathbf{0}]$ has a non-trivial solution. Here is the row reduction for the matrix  $$\\begin{aligned} &amp;\\begin{bmatrix} 1 &amp; 5 &amp; -3 \\\\ -1 &amp; -4 &amp; 1 \\\\ -2 &amp; -7 &amp; h \\end{bmatrix} \\quad \\xrightarrow{\\, R_2 \\mapsto R_2 + R_1 \\,,\\, R_3 \\mapsto R_3 + 2R_1 \\,} \\quad \\begin{bmatrix} 1 &amp; 5 &amp; -3 \\\\ 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 3 &amp; h - 6 \\end{bmatrix} \\\\[1em] &amp;\\xrightarrow{\\, R_3 \\mapsto R_3 - 3R_2 \\,} \\quad \\begin{bmatrix} 1 &amp; 5 &amp; -3 \\\\ 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; h \\end{bmatrix} \\end{aligned}$$  Since the last row has a pivot in the third column if $h \\neq 0$, the vectors are linearly independent. If $h = 0$, the last row becomes $[0 \\; 0 \\; 0]$, indicating that the vectors are linearly dependent. Thus, the only value of $h$ that makes the vectors linearly dependent is $h = 0$.</p> Question 9Let $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$ be a linear transformation such that $T\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\right) = \\begin{bmatrix} 2 \\\\ 5 \\end{bmatrix}$ and $T\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right) = \\begin{bmatrix} -1 \\\\ 6 \\end{bmatrix}$. What is $T\\left(\\begin{bmatrix} 3 \\\\ -2 \\end{bmatrix}\\right)$ (you will need to do some calculations!)?$\\begin{bmatrix} 1 \\\\ 11 \\end{bmatrix}$$\\begin{bmatrix} 8 \\\\ 3 \\end{bmatrix}$$\\begin{bmatrix} 4 \\\\ 27 \\end{bmatrix}$$\\begin{bmatrix} 5 \\\\ -7 \\end{bmatrix}$$\\begin{bmatrix} 6 \\\\ -10 \\end{bmatrix}$Check AnswerScore: 0 / 1<p>Explanation: Since $T$ is a linear transformation, $T(x+y) = T(x) + T(y)$ and $T(cx) = cT(x)$. We can write  $$\\begin{bmatrix} 3 \\\\ -2 \\end{bmatrix} = 3\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - 2\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$$ Therefore, $$\\begin{aligned} T\\left(\\begin{bmatrix} 3 \\\\ -2 \\end{bmatrix}\\right) &amp;= T\\left(3\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - 2\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right) \\\\ &amp;= 3T\\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\right) - 2T\\left(\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right) \\\\ &amp;= 3\\begin{bmatrix} 2 \\\\ 5 \\end{bmatrix} - 2\\begin{bmatrix} -1 \\\\ 6 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 6 \\\\ 15 \\end{bmatrix} - \\begin{bmatrix} -2 \\\\ 12 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 6 - (-2) \\\\ 15 - 12 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 8 \\\\ 3 \\end{bmatrix} \\end{aligned}$$  Alternatively, the standard matrix for $T$ is  $$A = \\begin{bmatrix} T(e_1) &amp; T(e_2) \\end{bmatrix} = \\begin{bmatrix} 2 &amp; -1 \\\\ 5 &amp; 6 \\end{bmatrix}$$  Then $$\\begin{aligned} T(x) = Ax &amp;= \\begin{bmatrix} 2 &amp; -1 \\\\ 5 &amp; 6 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ -2 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} (2)(3) + (-1)(-2) \\\\ (5)(3) + (6)(-2) \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 6+2 \\\\ 15-12 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 8 \\\\ 3 \\end{bmatrix} \\end{aligned}$$</p> Question 10If $A$ is an $m \\times n$ matrix whose columns span $\\mathbb{R}^m$, what must be true about the echelon form of $A$? (Revisiting concepts like Theorem 2 or 4 from Lay's book context)$A$ has a pivot position in every column.$A$ has $n$ rows.The equation $Ax = \\mathbf{0}$ has only the trivial solution.$m &lt; n$.$A$ has a pivot position in every row.Check AnswerScore: 0 / 1<p>Explanation: If the columns of an $m \\times n$ matrix $A$ span $\\mathbb{R}^m$, it means that for every $b$ in $\\mathbb{R}^m$, the equation $Ax = b$ has a solution. This is equivalent to stating that matrix $A$ has a pivot position in every row (according to Theorem 4 in Lay, etc., or similar theorems). This ensures there's no row of zeros in the coefficient part of the augmented matrix $[A|b]$ that could lead to an inconsistency like $0=c$ where $c \\neq 0$.</p>"},{"location":"01_Introduction_to_Linear_Algebra/#exercises","title":"Exercises","text":"<p>Exercise 1 (1.2.1)</p> <p>Determine which matrices are in reduced echelon form and which others are only in echelon form.</p> <ol> <li>\\(\\left[\\begin{array}{llll}1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{llll}1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{llll}1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{lllll}1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 2 &amp; 0 &amp; 2 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 3 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 4\\end{array}\\right]\\)</li> </ol> <ol> <li>Reduced echelon form</li> <li>Reduced echelon form</li> <li>Not echelon form</li> <li>Echelon form</li> </ol> <p>Exercise 2 (1.2.7)</p> <p>Find the general solution of the given system.</p> <p>\\(\\left[\\begin{array}{llll}1 &amp; 3 &amp; 4 &amp; 7 \\\\ 3 &amp; 9 &amp; 7 &amp; 6\\end{array}\\right]\\)</p> \\[ \\left\\{   \\begin{array}{l}     x_1 = -5 - 3 x_2 \\\\     x_2 \\text{ is free } \\\\     x_3 = 3   \\end{array} \\right. \\] <p>Exercise 3 (1.2.9)</p> <p>Find the general solution of the given system.</p> <p>\\(\\left[\\begin{array}{rrrr}0 &amp; 1 &amp; -2 &amp; 3 \\\\ 1 &amp; -3 &amp; 4 &amp; -6\\end{array}\\right]\\)</p> \\[ \\left\\{     \\begin{array}{l}         x_1=3+2 x_3 \\\\         x_2=3+2 x_3 \\\\         x_3 \\text { is free }     \\end{array}\\right. \\] <p>Exercise 4 (1.2.11)</p> <p>Find the general solution of the given system.</p> <p>\\(\\left[\\begin{array}{rrrr}3 &amp; -2 &amp; 4 &amp; 0 \\\\ 9 &amp; -6 &amp; 12 &amp; 0 \\\\ 6 &amp; -4 &amp; 8 &amp; 0\\end{array}\\right]\\)</p> \\[ \\left\\{     \\begin{array}{l}         x_1=\\frac{2}{3} x_2-\\frac{4}{3} x_3 \\\\         x_2 \\text { is free } \\\\         x_3 \\text { is free }     \\end{array}\\right. \\] <p>Exercise 5 (1.2.33)</p> <p>Find the interpolating polynomial \\(p(t)=a_0+a_1 t+a_2 t^2\\) for the data \\((1,6),(2,15),(3,28)\\). That is, find \\(a_0, a_1\\), and \\(a_2\\) such that</p> \\[ \\begin{aligned} &amp; a_0+a_1(1)+a_2(1)^2=6 \\\\ &amp; a_0+a_1(2)+a_2(2)^2=15 \\\\ &amp; a_0+a_1(3)+a_2(3)^2=28 \\end{aligned} \\] <p>The polynomial is \\(p(t)=1+3 t+2 t^2\\).</p> <p>Exercise 6 (1.3.11)</p> <p>Determine if \\(\\mathbf{b}\\) is a linear combination of \\(\\mathbf{a}_1, \\mathbf{a}_2\\), and \\(\\mathbf{a}_3\\).</p> <p>\\(\\mathbf{a}_1=\\left[\\begin{array}{r}1 \\\\ -2 \\\\ 0\\end{array}\\right], \\mathbf{a}_2=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 2\\end{array}\\right], \\mathbf{a}_3=\\left[\\begin{array}{r}5 \\\\ -6 \\\\ 8\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{r}2 \\\\ -1 \\\\ 6\\end{array}\\right]\\)</p> <p>\\(\\mathbf{b}\\) is a linear combination of \\(\\mathbf{a}_1, \\mathbf{a}_2\\), and \\(\\mathbf{a}_3\\).</p> <p>Exercise 7 (1.3.13)</p> <p>Determine if \\(\\mathbf{b}\\) is a linear combination of the vectors formed from the columns of the matrix \\(A\\).</p> <p>\\(A=\\left[\\begin{array}{rrr}1 &amp; -4 &amp; 2 \\\\ 0 &amp; 3 &amp; 5 \\\\ -2 &amp; 8 &amp; -4\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{r}3 \\\\ -7 \\\\ -3\\end{array}\\right]\\)</p> <p>\\(\\mathbf{b}\\) is not a linear combination of the columns of \\(A\\).</p> <p>Exercise 8 (1.3.14)</p> <p>Determine if \\(\\mathbf{b}\\) is a linear combination of the vectors formed from the columns of the matrix \\(A\\).</p> <p>\\(A=\\left[\\begin{array}{rrr}1 &amp; 0 &amp; 5 \\\\ -2 &amp; 1 &amp; -6 \\\\ 0 &amp; 2 &amp; 8\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{r}2 \\\\ -1 \\\\ 6\\end{array}\\right]\\)</p> <p>\\(\\mathbf{b}\\) is a linear combination of the columns of \\(A\\).</p> <p>Exercise 9 (1.3.28)</p> <p>A steam plant burns two types of coal: anthracite (A) and bituminous (B). For each ton of A burned, the plant produces 27.6 million Btu of heat, 3100 grams (g) of sulfur dioxide, and 250 g of particulate matter (solid-particle pollutants). For each ton of B burned, the plant produces 30.2 million Btu, 6400 g of sulfur dioxide, and 360 g of particulate matter.</p> <ol> <li>How much heat does the steam plant produce when it burns \\(x_1\\) tons of A and \\(x_2\\) tons of B ?</li> <li>Suppose the output of the steam plant is described by a vector that lists the amounts of heat, sulfur dioxide, and particulate matter. Express this output as a linear combination of two vectors, assuming that the plant burns \\(x_1\\) tons of A and \\(x_2\\) tons of B .</li> <li>\\([\\mathbf{M}]\\) Over a certain time period, the steam plant produced 162 million Btu of heat, \\(23,610 \\mathrm{~g}\\) of sulfur dioxide, and 1623 g of particulate matter. Determine how many tons of each type of coal the steam plant must have burned. Include a vector equation as part of your solution.</li> </ol> <ol> <li>The amount of heat produced when the steam plant burns \\(x_1\\) tons of anthracite and \\(x_2\\) tons of bituminous coal is \\(27.6 x_1+30.2 x_2\\) million Btu.</li> <li>The total output produced by \\(x_1\\) tons of anthracite and \\(x_2\\) tons of bituminous coal is given by the vector \\(x_1\\left[\\begin{array}{c}27.6 \\\\ 3100 \\\\ 250\\end{array}\\right]+x_2\\left[\\begin{array}{c}30.2 \\\\ 6400 \\\\ 360\\end{array}\\right]\\).</li> <li>The steam plant burned \\(3.9\\) tons of anthracite coal and \\(1.8\\) tons of bituminous coal.</li> </ol> <p>Exercise 10 (1.4.6)</p> <p>Use the definition of \\(A \\mathbf{x}\\) to write the matrix equation as a vector equation, or vice versa.</p> <ol> <li>\\(\\left[\\begin{array}{rr}2 &amp; -3 \\\\ 3 &amp; 2 \\\\ 8 &amp; -5 \\\\ -2 &amp; 1\\end{array}\\right]\\left[\\begin{array}{r}-3 \\\\ 5\\end{array}\\right]=\\left[\\begin{array}{r}-21 \\\\ 1 \\\\ -49 \\\\ 11\\end{array}\\right]\\)</li> </ol> <p>\\(-3 \\cdot\\left[\\begin{array}{r}2 \\\\ 3 \\\\ 8 \\\\ -2\\end{array}\\right]+5 \\cdot\\left[\\begin{array}{r}-3 \\\\ 2 \\\\ -5 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{r}-21 \\\\ 1 \\\\ -49 \\\\ 11\\end{array}\\right]\\)</p> <p>Exercise 11 (1.4.8)</p> <p>Use the definition of \\(A \\mathbf{x}\\) to write the matrix equation as a vector equation, or vice versa.</p> <ol> <li>\\(z_1\\left[\\begin{array}{r}2 \\\\ -4\\end{array}\\right]+z_2\\left[\\begin{array}{r}-1 \\\\ 5\\end{array}\\right]+z_3\\left[\\begin{array}{r}-4 \\\\ 3\\end{array}\\right]+z_4\\left[\\begin{array}{l}0 \\\\ 2\\end{array}\\right]=\\left[\\begin{array}{r}5 \\\\ 12\\end{array}\\right]\\)</li> </ol> <p>\\(\\left[\\begin{array}{rrrr}2 &amp; -1 &amp; -4 &amp; 0 \\\\ -4 &amp; 5 &amp; 3 &amp; 2\\end{array}\\right]\\left[\\begin{array}{l}z_1 \\\\ z_2 \\\\ z_3 \\\\ z_4\\end{array}\\right]=\\left[\\begin{array}{r}5 \\\\ 12\\end{array}\\right]\\)</p> <p>Exercise 12 (1.4.11)</p> <p>Given \\(A\\) and \\(\\mathbf{b}\\), write the augmented matrix for the linear system that corresponds to the matrix equation \\(A \\mathbf{x}=\\mathbf{b}\\). Then solve the system and write the solution as a vector.</p> <p>\\(A=\\left[\\begin{array}{rrr}1 &amp; 3 &amp; -4 \\\\ 1 &amp; 5 &amp; 2 \\\\ -3 &amp; -7 &amp; 6\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{r}-2 \\\\ 4 \\\\ 12\\end{array}\\right]\\)</p> <p>\\(\\mathbf{x}=\\left[\\begin{array}{l}x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right]=\\left[\\begin{array}{r}-11 \\\\ 3 \\\\ 0\\end{array}\\right]\\)</p> <p>Exercise 13 (1.4.17, 1.4.18)</p> <p>Refer to the matrices \\(A\\) and \\(B\\) below. Make appropriate calculations that justify your answers and mention an appropriate theorem.</p> \\[ A=\\left[\\begin{array}{rrrr} 1 &amp; 3 &amp; 0 &amp; 3 \\\\ -1 &amp; -1 &amp; -1 &amp; 1 \\\\ 0 &amp; -4 &amp; 2 &amp; -8 \\\\ 2 &amp; 0 &amp; 3 &amp; -1 \\end{array}\\right] \\quad B=\\left[\\begin{array}{rrrr} 1 &amp; 4 &amp; 1 &amp; 2 \\\\ 0 &amp; 1 &amp; 3 &amp; -4 \\\\ 0 &amp; 2 &amp; 6 &amp; 7 \\\\ 2 &amp; 9 &amp; 5 &amp; -7 \\end{array}\\right] \\] <ol> <li>How many rows of \\(A\\) contain a pivot position? Does the equation \\(A \\mathbf{x}=\\mathbf{b}\\) have a solution for each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^4\\) ?</li> <li>Can every vector in \\(\\mathbb{R}^4\\) be written as a linear combination of the columns of the matrix \\(B\\) above? Do the columns of \\(B\\) span \\(\\mathbb{R}^3\\) ?</li> </ol> <ol> <li>\\(A \\mathbf{x}=\\mathbf{b}\\) does not have a solution for each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^4\\).</li> <li>Not all vectors in \\(\\mathbb{R}^4\\) can be written as a linear combination of the columns of \\(B\\). The columns of \\(B\\) certainly do not span \\(\\mathbb{R}^3\\).</li> </ol> <p>Exercise 14 (1.5.7)</p> <p>Describe all solutions of \\(A \\mathbf{x}=\\mathbf{0}\\) in parametric vector form, where \\(A\\) is row equivalent to the given matrix.</p> <p>\\(\\left[\\begin{array}{llll}1 &amp; 3 &amp; -3 &amp; 7 \\\\ 0 &amp; 1 &amp; -4 &amp; 5\\end{array}\\right]\\)</p> <p>\\(\\mathbf{x}=\\left[\\begin{array}{l}x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\\end{array}\\right]=\\left[\\begin{array}{c}-9 x_3+8 x_4 \\\\ 4 x_3-5 x_4 \\\\ x_3 \\\\ x_4\\end{array}\\right]=\\left[\\begin{array}{c}-9 x_3 \\\\ 4 x_3 \\\\ x_3 \\\\ 0\\end{array}\\right]+\\left[\\begin{array}{c}8 x_4 \\\\ -5 x_4 \\\\ 0 \\\\ x_4\\end{array}\\right]=x_3\\left[\\begin{array}{r}-9 \\\\ 4 \\\\ 1 \\\\ 0\\end{array}\\right]+x_4\\left[\\begin{array}{r}8 \\\\ -5 \\\\ 0 \\\\ 1\\end{array}\\right]\\)</p> <p>Exercise 15 (1.5.15)</p> <p>Describe and compare the solution sets of \\(x_1+5 x_2-\\) \\(3 x_3=0\\) and \\(x_1+5 x_2-3 x_3=-2\\).</p> <p>The solution set of the homogeneous equation is the plane through the origin in \\(\\mathbf{R}^3\\) spanned by \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The solution set of the nonhomogeneous equation is parallel to this plane and passes through the point \\(\\mathbf{p}=\\left[\\begin{array}{r}-2 \\\\ 0 \\\\ 0\\end{array}\\right]\\).</p> <p>Exercise 16 (1.5.23)</p> <p>Mark each statement True or False. Justify each answer.</p> <ol> <li>A homogeneous equation is always consistent.</li> <li>The equation \\(A \\mathbf{x}=\\mathbf{0}\\) gives an explicit description of its solution set.</li> <li>The homogeneous equation \\(A \\mathbf{x}=\\mathbf{0}\\) has the trivial solution if and only if the equation has at least one free variable.</li> <li>The equation \\(\\mathbf{x}=\\mathbf{p}+t \\mathbf{v}\\) describes a line through \\(\\mathbf{v}\\) parallel to \\(\\mathbf{p}\\).</li> <li>The solution set of \\(A \\mathbf{x}=\\mathbf{b}\\) is the set of all vectors of the form \\(\\mathbf{w}=\\mathbf{p}+\\mathbf{v}_h\\), where \\(\\mathbf{v}_h\\) is any solution of the equation \\(A \\mathbf{x}=\\mathbf{0}\\).</li> </ol> <ol> <li>True. See the first paragraph of the subsection titled Homogeneous Linear Systems.</li> <li>False. The equation \\(A \\mathbf{x}=\\mathbf{0}\\) gives an implicit description of its solution set. See the subsection entitled Parametric Vector Form.</li> <li>False. The equation Ax =0 always has the trivial solution. The box before Example 1 uses the word nontrivial instead of trivial.</li> <li>False. The line goes through \\(\\mathbf{p}\\) parallel to \\(\\mathbf{v}\\). See the paragraph that precedes Fig. 5.</li> <li>False. The solution set could be empty! The statement (from Theorem 6) is true only when there exists a vector \\(\\mathbf{p}\\) such that \\(A \\mathbf{p}=\\mathbf{b}\\).</li> </ol> <p>Exercise 17 (1.5.24)</p> <p>Mark each statement True or False. Justify each answer.</p> <ol> <li>A homogeneous system of equations can be inconsistent.</li> <li>If \\(\\mathbf{x}\\) is a nontrivial solution of \\(A \\mathbf{x}=\\mathbf{0}\\), then every entry in \\(\\mathbf{x}\\) is nonzero.</li> <li>The effect of adding \\(\\mathbf{p}\\) to a vector is to move the vector in a direction parallel to \\(\\mathbf{p}\\).</li> <li>The equation \\(A \\mathbf{x}=\\mathbf{b}\\) is homogeneous if the zero vector is a solution.</li> </ol> <ol> <li>False. The trivial solution is always a solution to a homogeneous system of linear equations.</li> <li>False. A nontrivial solution of \\(A \\mathbf{x}=\\mathbf{0}\\) is any nonzero \\(\\mathbf{x}\\) that satisfies the equation. See the sentence before Example 2.</li> <li>True. See the paragraph following Example 3.</li> <li>True. If the zero vector is a solution, then \\(\\mathbf{b}=A \\mathbf{x}=A \\mathbf{0}=\\mathbf{0}\\).</li> </ol> Total Score Summary <p>Your total score for all quizzes on this page is:                     0 /                     0 </p>"},{"location":"02_Independence_and_Linear_Transformations/","title":"02 Independence and Linear Transformations","text":"Independence and Linear Transformations"},{"location":"02_Independence_and_Linear_Transformations/#session-material","title":"Session Material:","text":"<p>Lay: </p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"02_Independence_and_Linear_Transformations/#session-description","title":"Session Description","text":""},{"location":"02_Independence_and_Linear_Transformations/#key-concepts","title":"Key Concepts","text":"<p>Learning Objectives</p>"},{"location":"02_Independence_and_Linear_Transformations/#exercises","title":"Exercises","text":""},{"location":"03_Matrix_Algebra/","title":"03 Matrix Algebra","text":"Matrix Algebra"},{"location":"03_Matrix_Algebra/#session-material","title":"Session Material:","text":"<p>Lay: 2.1-2.7 (2.4 + 2.6-2.7 self-study)</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"03_Matrix_Algebra/#session-description","title":"Session Description","text":"<p>Now that we\u2019ve learned to represent and solve systems with matrices, this session focuses on doing algebra with matrices themselves. We\u2019ll start with basic operations like addition, scalar multiplication, and then tackle matrix multiplication \u2014 both its definition and the practical row-column rule.</p> <p>We\u2019ll explore special matrices (zero, identity, diagonal), powers, and transposes, and discuss how matrix operations differ from regular arithmetic. Key theorems (1\u20133 for operations, 5\u20137 for inverses) will guide us through the rules. A central idea is the invertible matrix and to find it using row reduction.</p> <p>We\u2019ll finish with the Invertible Matrix Theorem (Theorem 8), which ties together many core ideas.</p>"},{"location":"03_Matrix_Algebra/#key-concepts","title":"Key Concepts","text":"<ul> <li>Matrix Operations</li> <li>Matrix Multiplication</li> <li>Special Matrices: Zero, Identity, Diagonal</li> <li>Matrix Powers</li> <li>Matrix Transpose</li> <li>Invertible Matrices</li> <li>Invertible Matrix Theorem</li> <li>Matrix Algebra</li> </ul> <p>Learning Objectives</p> <ul> <li>Perform matrix addition, scalar multiplication, and matrix multiplication using the row-column rule.</li> <li>Identify and use special matrices (zero, identity, diagonal) and their properties.</li> <li>Compute and interpret matrix inverses and transposes.</li> <li>Apply the Invertible Matrix Theorem</li> <li>Analyze and solve problems using matrix algebra concepts.</li> </ul>"},{"location":"03_Matrix_Algebra/#exercises","title":"Exercises","text":"<p>Exercise 1 (1.7.11)</p> <p>Find the value(s) of \\(h\\) for which the vectors are linearly dependent. Justify your answer.</p> <p>\\(\\left[\\begin{array}{r}2 \\\\ -2 \\\\ 4\\end{array}\\right],\\left[\\begin{array}{r}4 \\\\ -6 \\\\ 7\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ 2 \\\\ h\\end{array}\\right]\\)</p> <p>The vectors are linearly dependent if and only if \\(h=-4\\).</p> <p>Exercise 2 (1.7.15-1.7.20)</p> <p>Determine by inspection whether the vectors in Exercises a-f are linearly independent. Justify each answer.</p> <ol> <li>\\(\\left[\\begin{array}{l}5 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{l}2 \\\\ 8\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right],\\left[\\begin{array}{r}-1 \\\\ 7\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{r}2 \\\\ -4 \\\\ 8\\end{array}\\right],\\left[\\begin{array}{r}-3 \\\\ 6 \\\\ -12\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{r}5 \\\\ -3 \\\\ -1\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}-7 \\\\ 2 \\\\ 4\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{l}3 \\\\ 4\\end{array}\\right],\\left[\\begin{array}{r}-1 \\\\ 5\\end{array}\\right],\\left[\\begin{array}{l}3 \\\\ 5\\end{array}\\right],\\left[\\begin{array}{l}7 \\\\ 1\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{r}-8 \\\\ 12 \\\\ -4\\end{array}\\right],\\left[\\begin{array}{r}2 \\\\ -3 \\\\ -1\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{r}1 \\\\ 4 \\\\ -7\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ 5 \\\\ 3\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right]\\)</li> </ol> <ol> <li>The set is linearly dependent, by Theorem 8, because there are four vectors in the set but only two entries in each vector.</li> <li>The set is linearly dependent because the second vector is \\(-3 / 2\\) times the first vector.</li> <li>The set is linearly dependent, by Theorem 9, because the list of vectors contains a zero vector.</li> <li>The set is linearly dependent, by Theorem 8, because there are four vectors in the set but only two entries in each vector.</li> <li>The set is linearly independent because neither vector is a multiple of the other vector. [Two of the entries in the first vector are -4 times the corresponding entry in the second vector. But this multiple does not work for the third entries.]</li> <li>The set is linearly dependent, by Theorem 9, because the list of vectors contains a zero vector.</li> </ol> <p>Exercise 3 (1.7.41)</p> <p>[M] Use as many columns of \\(A\\) as possible to construct a matrix \\(B\\) with the property that the equation \\(B \\mathbf{x}=\\mathbf{0}\\) has only the trivial solution. Solve \\(B \\mathbf{x}=\\mathbf{0}\\) to verify your work.</p> <p>\\(A=\\left[\\begin{array}{rrrrr}3 &amp; -4 &amp; 10 &amp; 7 &amp; -4 \\\\ -5 &amp; -3 &amp; -7 &amp; -11 &amp; 15 \\\\ 4 &amp; 3 &amp; 5 &amp; 2 &amp; 1 \\\\ 8 &amp; -7 &amp; 23 &amp; 4 &amp; 15\\end{array}\\right]\\)</p> \\[ B=\\left[\\begin{array}{rrr} 3 &amp; -4 &amp; 7 \\\\ -5 &amp; -3 &amp; -11 \\\\ 4 &amp; 3 &amp; 2 \\\\ 8 &amp; -7 &amp; 4 \\end{array}\\right] . \\text { Other choices are possible. } \\] <p>Exercise 4 (1.8.3-1.8.6)</p> <p>In Exercises a-d, with \\(T\\) defined by \\(T(\\mathbf{x})=A \\mathbf{x}\\), find a vector \\(\\mathbf{x}\\) whose image under \\(T\\) is \\(\\mathbf{b}\\), and determine whether \\(\\mathbf{x}\\) is unique.</p> <ol> <li>\\(A=\\left[\\begin{array}{rrr}1 &amp; 0 &amp; -3 \\\\ -3 &amp; 1 &amp; 6 \\\\ 2 &amp; -2 &amp; -1\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{r}-2 \\\\ 3 \\\\ -1\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrr}1 &amp; -2 &amp; 3 \\\\ 0 &amp; 1 &amp; -3 \\\\ 2 &amp; -5 &amp; 6\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{l}-6 \\\\ -4 \\\\ -5\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrr}1 &amp; -5 &amp; -7 \\\\ -3 &amp; 7 &amp; 5\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{l}-2 \\\\ -2\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrr}1 &amp; -3 &amp; 2 \\\\ 3 &amp; -8 &amp; 8 \\\\ 0 &amp; 1 &amp; 2 \\\\ 1 &amp; 0 &amp; 8\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{r}1 \\\\ 6 \\\\ 3 \\\\ 10\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(\\mathbf{x}=\\left[\\begin{array}{l}7 \\\\ 6 \\\\ 1\\end{array}\\right]\\), unique solution</li> <li>\\(\\mathbf{x}=\\left[\\begin{array}{r}-17 \\\\ -7 \\\\ -1\\end{array}\\right]\\), unique solution</li> <li>The general solution is \\(\\mathbf{x}=\\left[\\begin{array}{c}x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right]=\\left[\\begin{array}{c}3-3 x_3 \\\\ 1-2 x_3 \\\\ x_3\\end{array}\\right]=\\left[\\begin{array}{l}3 \\\\ 1 \\\\ 0\\end{array}\\right]+x_3\\left[\\begin{array}{r}-3 \\\\ -2 \\\\ 1\\end{array}\\right]\\). For a particular solution, one might choose \\(x_3=0\\) and \\(\\mathbf{x}=\\left[\\begin{array}{l}3 \\\\ 1 \\\\ 0\\end{array}\\right]\\).</li> <li>The general solution is \\(\\mathbf{x}=\\left[\\begin{array}{c}x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right]=\\left[\\begin{array}{c}10-8 x_3 \\\\ 3-2 x_3 \\\\ x_3\\end{array}\\right]=\\left[\\begin{array}{r}10 \\\\ 3 \\\\ 0\\end{array}\\right]+x_3\\left[\\begin{array}{r}-8 \\\\ -2 \\\\ 1\\end{array}\\right]\\). For a particular solution, one might choose \\(x_3=0\\) and \\(\\mathbf{x}=\\left[\\begin{array}{c}10 \\\\ 3 \\\\ 0\\end{array}\\right]\\).</li> </ol> <p>Exercise 5 (1.8.10)</p> <p>Find all \\(\\mathbf{x}\\) in \\(\\mathbb{R}^4\\) that are mapped into the zero vector by the transformation \\(\\mathbf{x} \\mapsto A \\mathbf{x}\\) for the given matrix \\(A\\).</p> <p>\\(A=\\left[\\begin{array}{rrrr}3 &amp; 2 &amp; 10 &amp; -6 \\\\ 1 &amp; 0 &amp; 2 &amp; -4 \\\\ 0 &amp; 1 &amp; 2 &amp; 3 \\\\ 1 &amp; 4 &amp; 10 &amp; 8\\end{array}\\right]\\)</p> <p>\\(\\left\\{\\begin{array}{l}x_1=-2 x_3+4 x_4 \\\\ x_2=-2 x_3-3 x_4 \\\\ x_3 \\text { is free } \\\\ x_4 \\text { is free }\\end{array} \\quad \\quad \\mathbf{x}=\\left[\\begin{array}{c}-2 x_3 \\\\ -2 x_3 \\\\ x_3 \\\\ 0\\end{array}\\right]+\\left[\\begin{array}{c}4 x_4 \\\\ -3 x_4 \\\\ 0 \\\\ x_4\\end{array}\\right]=x_3\\left[\\begin{array}{r}-2 \\\\ -2 \\\\ 1 \\\\ 0\\end{array}\\right]+x_4\\left[\\begin{array}{r}4 \\\\ -3 \\\\ 0 \\\\ 1\\end{array}\\right]\\right.\\)</p> <p>Exercise 6 (1.9.15)</p> <p>Fill in the missing entries of the matrix, assuming that the equation holds for all values of the variables.</p> <p>\\(\\left[\\begin{array}{lll}? &amp; ? &amp; ? \\\\ ? &amp; ? &amp; ? \\\\ ? &amp; ? &amp; ?\\end{array}\\right]\\left[\\begin{array}{l}x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right]=\\left[\\begin{array}{c}2 x_1-4 x_2 \\\\ x_1-x_3 \\\\ -x_2+3 x_3\\end{array}\\right]\\)</p> <p>By inspection, \\(\\left[\\begin{array}{rrr}2 &amp; -4 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 3\\end{array}\\right]\\left[\\begin{array}{l}x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right]=\\left[\\begin{array}{c}2 x_1-4 x_2 \\\\ x_1-x_3 \\\\ -x_2+3 x_3\\end{array}\\right]\\)</p> <p>Exercise 7 (2.1.1-2.1.2)</p> <p>Compute each matrix sum or product if it is defined. If an expression is undefined, explain why. Let</p> \\[ \\begin{aligned} &amp; A=\\left[\\begin{array}{rrr} 2 &amp; 0 &amp; -1 \\\\ 4 &amp; -5 &amp; 2 \\end{array}\\right], \\quad B=\\left[\\begin{array}{rrr} 7 &amp; -5 &amp; 1 \\\\ 1 &amp; -4 &amp; -3 \\end{array}\\right], \\\\ &amp; C=\\left[\\begin{array}{rr} 1 &amp; 2 \\\\ -2 &amp; 1 \\end{array}\\right], \\quad D=\\left[\\begin{array}{rr} 3 &amp; 5 \\\\ -1 &amp; 4 \\end{array}\\right], \\quad E=\\left[\\begin{array}{r} -5 \\\\ 3 \\end{array}\\right] \\end{aligned} \\] <ol> <li>\\(-2 A, B-2 A, A C, C D\\)</li> <li>\\(A+3 B, 2 C-3 E, D B, E C\\)</li> </ol> <ol> <li>The product \\(A C\\) is not defined because the number of columns of \\(A\\) does not match the number of rows of \\(C.  C D=\\left[\\begin{array}{rr}1 &amp; 2 \\\\ -2 &amp; 1\\end{array}\\right]\\left[\\begin{array}{rr}3 &amp; 5 \\\\ -1 &amp; 4\\end{array}\\right]=\\left[\\begin{array}{rr}1 \\cdot 3+2(-1) &amp; 1 \\cdot 5+2 \\cdot 4 \\\\ -2 \\cdot 3+1(-1) &amp; -2 \\cdot 5+1 \\cdot 4\\end{array}\\right]=\\left[\\begin{array}{rr}1 &amp; 13 \\\\ -7 &amp; -6\\end{array}\\right]\\). For mental computation, the row-column rule is probably easier to use than the definition.</li> <li>The sum \\(2C-3E\\) is not defined because the entries do not match. The product \\(E C\\) is not defined because the number of columns of \\(E\\) does not match the number of rows of \\(C\\).</li> </ol> <p>Exercise 8 (2.1.10)</p> <p>Assume that each matrix expression is defined. That is, the sizes of the matrices (and vectors) involved \"match\" appropriately.</p> <p>Let \\(\\quad A=\\left[\\begin{array}{rr}3 &amp; -6 \\\\ -1 &amp; 2\\end{array}\\right], \\quad B=\\left[\\begin{array}{rr}-1 &amp; 1 \\\\ 3 &amp; 4\\end{array}\\right], \\quad\\) and \\(\\quad C=\\) \\(\\left[\\begin{array}{rr}-3 &amp; -5 \\\\ 2 &amp; 1\\end{array}\\right]\\). Verify that \\(A B=A C\\) and yet \\(B \\neq C\\).</p> <p>\\(A B=\\left[\\begin{array}{rr}3 &amp; -6 \\\\ -1 &amp; 2\\end{array}\\right]\\left[\\begin{array}{rr}-1 &amp; 1 \\\\ 3 &amp; 4\\end{array}\\right]=\\left[\\begin{array}{rr}-21 &amp; -21 \\\\ 7 &amp; 7\\end{array}\\right], A C=\\left[\\begin{array}{rr}3 &amp; -6 \\\\ -1 &amp; 2\\end{array}\\right]\\left[\\begin{array}{rr}-3 &amp; -5 \\\\ 2 &amp; 1\\end{array}\\right]=\\left[\\begin{array}{rr}-21 &amp; -21 \\\\ 7 &amp; 7\\end{array}\\right]\\)</p> <p>Exercise 9 (2.1.13)</p> <p>Assume that each matrix expression is defined. That is, the sizes of the matrices (and vectors) involved \"match\" appropriately.</p> <p>Let \\(\\mathbf{r}_1, \\ldots, \\mathbf{r}_p\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(Q\\) be an \\(m \\times n\\) matrix. Write the matrix \\(\\left[\\begin{array}{lll}Q \\mathbf{r}_1 &amp; \\cdots &amp; Q \\mathbf{r}_p\\end{array}\\right]\\) as a product of two matrices (neither of which is an identity matrix).</p> <p>Use the definition of \\(A B\\) written in reverse order: \\(\\left[A \\mathbf{b}_1 \\cdots A \\mathbf{b}_p\\right]=A\\left[\\mathbf{b}_1 \\cdots \\mathbf{b}_p\\right]\\). Thus \\(\\left[Q \\mathbf{r}_1 \\cdots Q \\mathbf{r}_p\\right]=Q R\\), when \\(R=\\left[\\begin{array}{lll}\\mathbf{r}_1 &amp; \\cdots &amp; \\mathbf{r}_p\\end{array}\\right]\\).</p> <p>Exercise 10 (2.1.40)</p> <p>\\([\\mathbf{M}]\\) Let</p> \\[ S=\\left[\\begin{array}{lllll} 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right] \\] <p>Compute \\(S^k\\) for \\(k=2, \\ldots, 6\\).</p> <p>\\(S^2=\\left[\\begin{array}{lllll}0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\)</p> <p>\\(S^3=\\left[\\begin{array}{lllll}0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\)</p> <p>\\(S^4=\\left[\\begin{array}{lllll}0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\)</p> <p>\\(S^5\\) is the \\(5 \\times 5\\) zero matrix.   </p> <p>\\(S^6\\) is also the \\(5 \\times 5\\) zero matrix.</p> <p>Exercise 11 (2.1.41)</p> <p>Assume that each matrix expression is defined. That is, the sizes of the matrices (and vectors) involved \"match\" appropriately.</p> <p>\\([\\mathbf{M}]\\) Describe in words what happens when \\(A^5, A^{10}, A^{20}\\), and \\(A^{30}\\) are computed for</p> \\[ A=\\left[\\begin{array}{rrr} 1 / 4 &amp; 1 / 2 &amp; 1 / 4 \\\\ 1 / 2 &amp; 1 / 3 &amp; 1 / 6 \\\\ 1 / 4 &amp; 1 / 6 &amp; 7 / 12 \\end{array}\\right] \\] <p>The entries in \\(A^{20}\\) all agree with .3333333333 to 8 or 9 decimal places. The entries in \\(A^{30}\\) all agree with .33333333333333 to at least 14 decimal places. The matrices appear to approach the matrix \\(\\left[\\begin{array}{lll}1 / 3 &amp; 1 / 3 &amp; 1 / 3 \\\\ 1 / 3 &amp; 1 / 3 &amp; 1 / 3 \\\\ 1 / 3 &amp; 1 / 3 &amp; 1 / 3\\end{array}\\right]\\). Further exploration of this behavior appears in Sections 4.9 and 5.2.</p> <p>Exercise 12 (2.2.9)</p> <p>Mark each statement True or False. Justify each answer.</p> <ol> <li>In order for a matrix \\(B\\) to be the inverse of \\(A\\), the equations \\(A B=I\\) and \\(B A=I\\) must both be true.</li> <li>If \\(A\\) and \\(B\\) are \\(n \\times n\\) and invertible, then \\(A^{-1} B^{-1}\\) is the inverse of \\(A B\\).</li> <li>If \\(A=\\left[\\begin{array}{ll}a &amp; b \\\\ c &amp; d\\end{array}\\right]\\) and \\(a b-c d \\neq 0\\), then \\(A\\) is invertible.</li> <li>If \\(A\\) is an invertible \\(n \\times n\\) matrix, then the equation \\(A \\mathbf{x}=\\mathbf{b}\\) is consistent for each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^n\\).</li> <li>Each elementary matrix is invertible.</li> </ol> <ol> <li>True, by definition of invertible.</li> <li>False. See Theorem 6(b).</li> <li>False. If \\(A=\\left[\\begin{array}{ll}1 &amp; 1 \\\\ 0 &amp; 0\\end{array}\\right]\\), then \\(a b-c d=1-0 \\neq 0\\), but Theorem 4 shows that this matrix is not invertible, because \\(a d-b c=0\\).</li> <li>True. This follows from Theorem 5, which also says that the solution of \\(A \\mathbf{x}=\\mathbf{b}\\) is unique, for each \\(\\mathbf{b}\\).</li> <li>True, by the box just before Example 6 .</li> </ol> <p>Exercise 13 (2.2.30-2.2.32)</p> <p>Find the inverses of the matrices, if they exist. Use the algorithm introduced in this section.</p> <ol> <li>\\(\\left[\\begin{array}{ll}3 &amp; 6 \\\\ 4 &amp; 7\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{rrr}1 &amp; 0 &amp; -2 \\\\ -3 &amp; 1 &amp; 4 \\\\ 2 &amp; -3 &amp; 4\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{rrr}1 &amp; 2 &amp; -1 \\\\ -4 &amp; -7 &amp; 3 \\\\ -2 &amp; -6 &amp; 4\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(A^{-1}=\\left[\\begin{array}{rr}-7 / 3 &amp; 2 \\\\ 4 / 3 &amp; -1\\end{array}\\right]\\)</li> <li>\\(A^{-1}=\\left[\\begin{array}{ccc}8 &amp; 3 &amp; 1 \\\\ 10 &amp; 4 &amp; 1 \\\\ 7 / 2 &amp; 3 / 2 &amp; 1 / 2\\end{array}\\right]\\)</li> <li>The matrix \\(A\\) is not invertible.</li> </ol> <p>Exercise 14 (2.3.11)</p> <p>The matrices are all \\(n \\times n\\). Each part of the exercises is an implication of the form \"If \u3008statement 1\u3009, then \\(\\langle\\) statement 2\\(\\rangle\\).\" Mark an implication as True if the truth of\u3008statement 2\u3009 always follows whenever \u3008statement 1\u3009 happens to be true. An implication is False if there is an instance in which \u3008statement 2 \u3009 is false but \\(\\langle\\) statement 1\\(\\rangle\\) is true. Justify each answer.</p> <ol> <li>If the equation \\(A \\mathbf{x}=\\mathbf{0}\\) has only the trivial solution, then \\(A\\) is row equivalent to the \\(n \\times n\\) identity matrix.</li> <li>If the columns of \\(A\\) span \\(\\mathbb{R}^n\\), then the columns are linearly independent.</li> <li>If \\(A\\) is an \\(n \\times n\\) matrix, then the equation \\(A \\mathbf{x}=\\mathbf{b}\\) has at least one solution for each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^n\\).</li> <li>If the equation \\(A \\mathbf{x}=\\mathbf{0}\\) has a nontrivial solution, then \\(A\\) has fewer than \\(n\\) pivot positions.</li> <li>If \\(A^T\\) is not invertible, then \\(A\\) is not invertible.</li> </ol> <ol> <li>True, by the IMT. If statement (d) of the IMT is true, then so is statement (b).</li> <li>True. If statement (h) of the IMT is true, then so is statement (e).</li> <li>False. Statement (g) of the IMT is true only for invertible matrices.</li> <li>True, by the IMT. If the equation \\(A \\mathbf{x}=\\mathbf{0}\\) has a nontrivial solution, then statement (d) of the IMT is false. In this case, all the lettered statements in the IMT are false, including statement \u00a9, which means that \\(A\\) must have fewer than \\(n\\) pivot positions.</li> <li>True, by the IMT. If \\(A^T\\) is not invertible, then statement (1) of the IMT is false, and hence statement (a) must also be false.</li> </ol> <p>Exercise 15 (2.3.12)</p> <p>The matrices are all \\(n \\times n\\). Each part of the exercises is an implication of the form \"If \u3008statement 1\u3009, then \\(\\langle\\) statement 2\\(\\rangle\\).\" Mark an implication as True if the truth of\u3008statement 2\u3009 always follows whenever \u3008statement 1\u3009 happens to be true. An implication is False if there is an instance in which \u3008statement 2 \u3009 is false but \\(\\langle\\) statement 1\\(\\rangle\\) is true. Justify each answer.</p> <ol> <li>If there is an \\(n \\times n\\) matrix \\(D\\) such that \\(A D=I\\), then \\(D A=I\\).</li> <li>If the linear transformation \\(\\mathbf{x} \\mapsto A \\mathbf{x}\\) maps \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}^n\\), then the row reduced echelon form of \\(A\\) is \\(I\\).</li> <li>If the columns of \\(A\\) are linearly independent, then the columns of \\(A\\) span \\(\\mathbb{R}^n\\).</li> <li>If the equation \\(A \\mathbf{x}=\\mathbf{b}\\) has at least one solution for each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^n\\), then the transformation \\(\\mathbf{x} \\mapsto A \\mathbf{x}\\) is not one-to-one.</li> <li>If there is a \\(\\mathbf{b}\\) in \\(\\mathbb{R}^n\\) such that the equation \\(A \\mathbf{x}=\\mathbf{b}\\) is consistent, then the solution is unique.</li> </ol> <ol> <li>True. If statement \\((\\mathrm{k})\\) of the IMT is true, then so is statement \\((\\mathrm{j})\\). Use the first box after the IMT.</li> <li>False. Notice that (i) if the IMT uses the work onto rather than the word into.</li> <li>True. If statement (e) of the IMT is true, then so is statement (h).</li> <li>False. Since (g) if the IMT is true, so is (f).</li> <li>False, by the IMT. The fact that there is a \\(\\mathbf{b}\\) in \\(\\mathbb{R}^n\\) such that the equation \\(A \\mathbf{x}=\\mathbf{b}\\) is consistent, does not imply that statement (g) of the IMT is true, and hence there could be more than one solution.</li> </ol> <p>Exercise 16 (2.3.15)</p> <p>Is it possible for a \\(4 \\times 4\\) matrix to be invertible when its columns do not span \\(\\mathbb{R}^4\\) ? Why or why not?</p> <p>Part (h) of the IMT shows that a \\(4 \\times 4\\) matrix cannot be invertible when its columns do not span \\(\\mathbf{R}^4\\).</p> <p>Exercise 17 (2.3.17)</p> <p>Can a square matrix with two identical columns be invertible? Why or why not?</p> <p>If \\(A\\) has two identical columns then its columns are linearly dependent. Part (e) of the IMT shows that \\(A\\) cannot be invertible.</p>"},{"location":"04_Determinants/","title":"04 Determinants","text":"Determinants"},{"location":"04_Determinants/#session-material","title":"Session Material:","text":"<p>Lay: 3.1-3.3</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"04_Determinants/#session-description","title":"Session Description","text":"<p>This session introduces the concept of the determinant of a matrix. We will start by defining and learning how to compute the determinant for \\(2 \\times 2\\) matrices, and then extend this to \\(3 \\times 3\\) matrices and  \\(n \\times n\\) matrices, using a systematic method.</p> <p>We will explore the basic properties of the determinant, including how row operations affect its value. A central result that will be highlighted is the relationship between the determinant and a matrix's invertibility \u2013 a matrix is invertible if and only if its determinant is non-zero.</p> <p>Depending on the precise content of the sections, we may also touch upon the geometric interpretation of the determinant, such as its connection to area or volume.</p>"},{"location":"04_Determinants/#key-concepts","title":"Key Concepts","text":"<ul> <li>Determinant</li> <li>Calculating 2x2 Determinants</li> <li>Calculating 3x3 Determinants</li> <li>Properties of Determinants</li> <li>Determinant and Invertibility</li> <li>Geometric Interpretation (Area/Volume)</li> </ul> <p>Learning Objectives</p> <ul> <li>Compute determinants of \\(2 \\times 2\\), \\(3 \\times 3\\), and \\(n \\times n\\) matrices using systematic methods.</li> <li>Apply properties of determinants to simplify calculations and understand matrix behavior.</li> <li>Relate the determinant to matrix invertibility and solve related problems.</li> <li>Interpret the geometric meaning of determinants in terms of area and volume.</li> <li>Analyze the effect of row operations on the determinant.</li> </ul>"},{"location":"04_Determinants/#exercises","title":"Exercises","text":"<p>Exercise 1 (3.1.1-3.1.2)</p> <p>Compute the determinants using a cofactor expansion across the first row. Then compute the determinant also by a cofactor expansion down the second column.</p> <ol> <li>\\(\\left|\\begin{array}{rrr}3 &amp; 0 &amp; 4 \\\\ 2 &amp; 3 &amp; 2 \\\\ 0 &amp; 5 &amp; -1\\end{array}\\right|\\)</li> <li>\\(\\left|\\begin{array}{rrr}0 &amp; 5 &amp; 1 \\\\ 4 &amp; -3 &amp; 0 \\\\ 2 &amp; 4 &amp; 1\\end{array}\\right|\\)</li> </ol> <ol> <li>1</li> <li>2</li> </ol> <p>Exercise 2 (3.1.9)</p> <p>Compute the determinant by cofactor expansions. At each step, choose a row or column that involves the least amount of computation.</p>  $\\displaystyle \\left|\\begin{array}{rrrr} 6 &amp; 0 &amp; 0 &amp; 5 \\\\ 1 &amp; 7 &amp; 2 &amp; -5 \\\\ 2 &amp; 0 &amp; 0 &amp; 0 \\\\ 8 &amp; 3 &amp; 1 &amp; 8 \\end{array}\\right|$  <p>10</p> <p>Exercise 3 (3.1.43)</p> <p>\\([\\mathbf{M}]\\) Is it true that \\(\\operatorname{det}(A+B)=\\operatorname{det} A+\\operatorname{det} B ?\\) To find out, generate random \\(5 \\times 5\\) matrices \\(A\\) and \\(B\\), and compute \\(\\operatorname{det}(A+B)-\\operatorname{det} A-\\operatorname{det} B\\). (Refer to Exercise 37 in Section 2.1.) Repeat the calculations for three other pairs of \\(n \\times n\\) matrices, for various values of \\(n\\). Report your results.</p> <p>Here are sample results testing whether \\(\\det(A+B)=\\det A+\\det B\\) for random integer matrices \\(A,B\\) of sizes \\(n=5,2,3,10\\):</p> n det(A) det(B) det(A + B) det(A + B) \u2212 det(A) \u2212 det(B) 5 668 \u22121077 2765 3174 2 \u221222 0 \u221226 \u22124 3 \u221293 28 \u221227 38 10 3.11 \u00d7 10\u2076 3.35 \u00d7 10\u2077 3.28 \u00d7 10\u2078 2.92 \u00d7 10\u2078 <p>In every case, \\(\\det(A+B)-\\det A-\\det B\\neq0\\), demonstrating that \\(\\det(A+B)\\neq\\det A+\\det B\\). In general, determinant is not an additive function of matrices.</p> <p>Exercise 4 (3.2.1-3.2.2)</p> <p>Both equations illustrate a property of determinants. State the property.</p> <ol> <li>\\(\\left|\\begin{array}{rrr}0 &amp; 5 &amp; -2 \\\\ 1 &amp; -3 &amp; 6 \\\\ 4 &amp; -1 &amp; 8\\end{array}\\right|=-\\left|\\begin{array}{rrr}1 &amp; -3 &amp; 6 \\\\ 0 &amp; 5 &amp; -2 \\\\ 4 &amp; -1 &amp; 8\\end{array}\\right|\\)</li> <li>\\(\\left|\\begin{array}{rrr}2 &amp; -6 &amp; 4 \\\\ 3 &amp; 5 &amp; -2 \\\\ 1 &amp; 6 &amp; 3\\end{array}\\right|=2\\left|\\begin{array}{rrr}1 &amp; -3 &amp; 2 \\\\ 3 &amp; 5 &amp; -2 \\\\ 1 &amp; 6 &amp; 3\\end{array}\\right|\\)</li> </ol> <ol> <li>Rows 1 and 2 are interchanged, so the determinant changes sign (Theorem 3b.).</li> <li>The constant 2 may be factored out of the Row 1 (Theorem 3c.).</li> </ol> <p>Exercise 5 (3.2.15-3.2.20)</p> <p>Find the determinants in the following exercises, where</p> \\[ \\left|\\begin{array}{lll} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{array}\\right|=7 . \\] <ol> <li>\\(\\left|\\begin{array}{ccc}a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ 5 g &amp; 5 h &amp; 5 i\\end{array}\\right|\\)</li> <li>\\(\\left|\\begin{array}{ccc}a &amp; b &amp; c \\\\ 3 d &amp; 3 e &amp; 3 f \\\\ g &amp; h &amp; i\\end{array}\\right|\\)</li> <li>\\(\\left|\\begin{array}{lll}a &amp; b &amp; c \\\\ g &amp; h &amp; i \\\\ d &amp; e &amp; f\\end{array}\\right|\\)</li> <li>\\(\\left|\\begin{array}{lll}g &amp; h &amp; i \\\\ a &amp; b &amp; c \\\\ d &amp; e &amp; f\\end{array}\\right|\\)</li> <li>\\(\\left|\\begin{array}{ccc}a &amp; b &amp; c \\\\ 2 d+a &amp; 2 e+b &amp; 2 f+c \\\\ g &amp; h &amp; i\\end{array}\\right|\\)</li> <li>\\(\\left|\\begin{array}{ccc}a+d &amp; b+e &amp; c+f \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i\\end{array}\\right|\\)</li> </ol> <ol> <li>35</li> <li>21</li> <li>-7</li> <li>7</li> <li>14</li> <li>7</li> </ol> <p>Exercise 6 (3.2.25)</p> <p>Use the determinant to decide if the set of vectors is linearly independent.</p>  $\\left[\\begin{array}{r}7 \\\\ -4 \\\\ -6\\end{array}\\right],\\left[\\begin{array}{r}-8 \\\\ 5 \\\\ 7\\end{array}\\right],\\left[\\begin{array}{r}7 \\\\ 0 \\\\ -5\\end{array}\\right]$  <p>The columns of the matrix form a linearly independent set.</p>"},{"location":"05_Vector_Spaces/","title":"05 Vector Spaces","text":"Vector Spaces"},{"location":"05_Vector_Spaces/#session-material","title":"Session Material:","text":"<p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"05_Vector_Spaces/#session-description","title":"Session Description","text":"<p>Moving beyond just vectors in \\(\\mathbb{R}^n\\), this session introduces the more general idea of \"vector spaces\" \u2013 abstract collections of objects that behave like vectors. We'll look at special subsets within these spaces called \"subspaces.\" A major focus will be on two important subspaces connected to any matrix: the \"Null Space\" (all the vectors that get mapped to zero) and the \"Column Space\" (everything that can be reached by combining the matrix's columns). We'll also touch on how these relate to linear transformations.</p> <p>A crucial concept we'll build up to is finding the most efficient way to describe or generate a space \u2013 this leads us to the idea of a \"basis,\" which is a minimal set of vectors that spans the entire space and is linearly independent. Once we have a basis, we can define unique \"coordinate systems\" within the space. We'll then use the basis to define the \"dimension\" of a vector space \u2013 essentially, how many independent directions you need to move within it. Finally, we'll define the \"rank\" of a matrix, which is closely tied to the dimension of its column space and tells us a lot about the structure of the matrix and related linear systems.</p>"},{"location":"05_Vector_Spaces/#key-concepts","title":"Key Concepts","text":"<ul> <li>Vector Spaces</li> <li>Subspaces</li> <li>Null Space</li> <li>Column Space</li> <li>Basis</li> <li>Coordinate Systems</li> <li>Dimension</li> <li>Rank</li> </ul> <p>Learning Objectives</p> <ul> <li>Define and identify vector spaces and subspaces.</li> <li>Determine and construct the null space and column space of a matrix.</li> <li>Construct and use a basis to describe vector spaces and coordinate systems.</li> <li>Calculate the dimension and rank of vector spaces and matrices.</li> <li>Analyze the relationship between basis, dimension, and solutions to linear systems.</li> </ul>"},{"location":"05_Vector_Spaces/#exercises","title":"Exercises","text":"<p>Exercise 1 (4.2.2)</p> <p>Determine if \\(w=\\left[\\begin{array}{r}1 \\\\ -1 \\\\ 1\\end{array}\\right]\\) is in \\(\\operatorname{Nul} A\\), where</p> \\[ A=\\left[\\begin{array}{rrr} 2 &amp; 6 &amp; 4 \\\\ -3 &amp; 2 &amp; 5 \\\\ -5 &amp; -4 &amp; 1 \\end{array}\\right] \\] <p>\\(\\mathbf{w}\\) is in \\(\\operatorname{Nul} A\\)</p> <p>Exercise 2 (4.2.3-4.2.6)</p> <p>In the following exercises, find an explicit description of Nul A, by listing vectors that span the null space.</p> <ol> <li>\\(A=\\left[\\begin{array}{rrrr}1 &amp; 2 &amp; 4 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 &amp; -2\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrrr}1 &amp; -3 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 &amp; 0\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrrrr}1 &amp; -4 &amp; 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrrrr}1 &amp; 3 &amp; -4 &amp; -3 &amp; 1 \\\\ 0 &amp; 1 &amp; -3 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(\\left\\{\\left[\\begin{array}{r}2 \\\\ -3 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}-4 \\\\ 2 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\)</li> <li>\\(\\left\\{\\left[\\begin{array}{l}3 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\)</li> <li>\\(\\left\\{\\left[\\begin{array}{l}4 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ 0 \\\\ 5 \\\\ 1 \\\\ 0\\end{array}\\right]\\right\\}\\)</li> <li>\\(\\left\\{\\left[\\begin{array}{r}-5 \\\\ 3 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}6 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}-1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\)</li> </ol> <p>Exercise 3 (4.2.17-4.2.20)</p> <p>For the matrices in the following exercises, (a) find \\(k\\) such that \\(\\mathrm{Nul} A\\) is a subspace of \\(\\mathbb{R}^k\\), and (b) find \\(k\\) such that \\(\\operatorname{Col} A\\) is a subspace of \\(\\mathbb{R}^k\\).</p> <ol> <li>\\(A=\\left[\\begin{array}{rr}6 &amp; -4 \\\\ -3 &amp; 2 \\\\ -9 &amp; 6 \\\\ 9 &amp; -6\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrr}5 &amp; -2 &amp; 3 \\\\ -1 &amp; 0 &amp; -1 \\\\ 0 &amp; -2 &amp; -2 \\\\ -5 &amp; 7 &amp; 2\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrrrr}4 &amp; 5 &amp; -2 &amp; 6 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{lllll}1 &amp; -3 &amp; 2 &amp; 0 &amp; -5\\end{array}\\right]\\)</li> </ol> <ol> <li>The matrix \\(A\\) is a \\(4 \\times 2\\) matrix. Thus     (a) \\(\\operatorname{Nul} A\\) is a subspace of \\(\\mathbb{R}^2\\), and     (b) \\(\\operatorname{Col} A\\) is a subspace of \\(\\mathbb{R}^4\\).</li> <li>The matrix \\(A\\) is a \\(4 \\times 3\\) matrix. Thus     (a) \\(\\operatorname{Nul} A\\) is a subspace of \\(\\mathbb{R}^3\\), and     (b) \\(\\operatorname{Col} A\\) is a subspace of \\(\\mathbb{R}^4\\).</li> <li>The matrix \\(A\\) is a \\(2 \\times 5\\) matrix. Thus     (a) \\(\\operatorname{Nul} A\\) is a subspace of \\(\\mathbb{R}^5\\), and     (b) \\(\\operatorname{Col} A\\) is a subspace of \\(\\mathbb{R}^2\\).</li> <li>The matrix \\(A\\) is a \\(1 \\times 5\\) matrix. Thus     (a) \\(\\operatorname{Nul} A\\) is a subspace of \\(\\mathbb{R}^5\\), and     (b) \\(\\operatorname{Col} A\\) is a subspace of \\(\\mathbb{R}^1=\\mathbb{R}\\).</li> </ol> <p>Exercise 4 (4.2.24)</p> <p>Let \\(A=\\left[\\begin{array}{rrrr}10 &amp; -8 &amp; -2 &amp; -2 \\\\ 0 &amp; 2 &amp; 2 &amp; -2 \\\\ 1 &amp; -1 &amp; 6 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; -2\\end{array}\\right]\\) and \\(\\mathbf{w}=\\left[\\begin{array}{l}2 \\\\ 2 \\\\ 0 \\\\ 2\\end{array}\\right]\\). Determine if \\(\\mathbf{w}\\) is in \\(\\operatorname{Col} A\\). Is \\(\\mathbf{w}\\) in \\(\\operatorname{Nul} A\\) ?</p> <p>\\(\\mathbf{w}\\) is in \\(\\operatorname{Nul} A\\)</p> <p>Exercise 5 (4.3.9-4.3.10)</p> <p>Find bases for the null spaces of the given matrices. Refer to the remarks that follow Example 3 in Section 4.2.</p> <ol> <li>\\(\\left[\\begin{array}{rrrr}1 &amp; 0 &amp; -2 &amp; -2 \\\\ 0 &amp; 1 &amp; 1 &amp; 4 \\\\ 3 &amp; -1 &amp; -7 &amp; 3\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{rrrrr}1 &amp; 1 &amp; -2 &amp; 1 &amp; 5 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 &amp; -2 \\\\ 0 &amp; 0 &amp; -8 &amp; 0 &amp; 16\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(\\left\\{\\left[\\begin{array}{r}2 \\\\ -1 \\\\ 1 \\\\ 0\\end{array}\\right]\\right\\}\\)</li> <li>\\(\\left\\{\\left[\\begin{array}{r}-2 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}9 \\\\ -10 \\\\ 2 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\)</li> </ol> <p>Exercise 6 (4.3.13)</p> <p>Assume that \\(A\\) is row equivalent to \\(B\\). Find the base for \\(\\operatorname{Nul} A\\) and \\(\\operatorname{Col} A\\).</p> <p>\\(A=\\left[\\begin{array}{rrrr}-2 &amp; 4 &amp; -2 &amp; -4 \\\\ 2 &amp; -6 &amp; -3 &amp; 1 \\\\ -3 &amp; 8 &amp; 2 &amp; -3\\end{array}\\right], B=\\left[\\begin{array}{llll}1 &amp; 0 &amp; 6 &amp; 5 \\\\ 0 &amp; 2 &amp; 5 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\)</p> <p>\\(\\left\\{\\left[\\begin{array}{r}-6 \\\\ -5 / 2 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}-5 \\\\ -3 / 2 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\)</p> <p>Exercise 7 (4.3.17-4.3.18)</p> <p>Find a basis for the space spanned by the given vectors, \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_5\\).</p> <ol> <li>\\([\\mathbf{M}]\\left[\\begin{array}{r}2 \\\\ 0 \\\\ -4 \\\\ -6 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}4 \\\\ 0 \\\\ 2 \\\\ -4 \\\\ 4\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ -4 \\\\ 0 \\\\ 1 \\\\ -7\\end{array}\\right],\\left[\\begin{array}{r}8 \\\\ 4 \\\\ 8 \\\\ -3 \\\\ 15\\end{array}\\right],\\left[\\begin{array}{r}-8 \\\\ 4 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]\\)</li> <li>\\([\\mathbf{M}]\\left[\\begin{array}{r}-3 \\\\ 2 \\\\ 6 \\\\ 0 \\\\ -7\\end{array}\\right],\\left[\\begin{array}{r}3 \\\\ 0 \\\\ -9 \\\\ 0 \\\\ 6\\end{array}\\right],\\left[\\begin{array}{r}0 \\\\ 2 \\\\ -4 \\\\ 0 \\\\ -1\\end{array}\\right],\\left[\\begin{array}{r}6 \\\\ -2 \\\\ -14 \\\\ 0 \\\\ 13\\end{array}\\right],\\left[\\begin{array}{r}-6 \\\\ 3 \\\\ 0 \\\\ -1 \\\\ 0\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(\\left\\{\\left[\\begin{array}{r}2 \\\\ 0 \\\\ -4 \\\\ -6 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}4 \\\\ 0 \\\\ 2 \\\\ -4 \\\\ 4\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ -4 \\\\ 0 \\\\ 1 \\\\ -7\\end{array}\\right],\\left[\\begin{array}{r}-8 \\\\ 4 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\)</li> <li>\\(\\left\\{\\left[\\begin{array}{r}-3 \\\\ 2 \\\\ 6 \\\\ 0 \\\\ -7\\end{array}\\right],\\left[\\begin{array}{r}3 \\\\ 0 \\\\ -9 \\\\ 0 \\\\ 6\\end{array}\\right],\\left[\\begin{array}{r}0 \\\\ 2 \\\\ -4 \\\\ 0 \\\\ -1\\end{array}\\right],\\left[\\begin{array}{r}-6 \\\\ 3 \\\\ 0 \\\\ -1 \\\\ 0\\end{array}\\right]\\right\\}\\)</li> </ol> <p>Exercise 8 (4.3.36)</p> <p>[M] Let \\(H=\\operatorname{Span}\\left\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\right\\}\\) and \\(K=\\operatorname{Span}\\left\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\right\\}\\), where</p> \\[ \\begin{aligned} \\mathbf{u}_1 &amp;= \\begin{bmatrix}1\\\\2\\\\0\\\\-1\\end{bmatrix}, &amp; \\mathbf{u}_2 &amp;= \\begin{bmatrix}0\\\\2\\\\-1\\\\1\\end{bmatrix}, &amp; \\mathbf{u}_3 &amp;= \\begin{bmatrix}3\\\\4\\\\1\\\\-4\\end{bmatrix},\\\\ \\mathbf{v}_1 &amp;= \\begin{bmatrix}-2\\\\-2\\\\-1\\\\3\\end{bmatrix}, &amp; \\mathbf{v}_2 &amp;= \\begin{bmatrix}2\\\\3\\\\2\\\\-6\\end{bmatrix}, &amp; \\mathbf{v}_3 &amp;= \\begin{bmatrix}-1\\\\4\\\\6\\\\-2\\end{bmatrix}. \\end{aligned} \\] <p>Find bases for \\(H, K\\), and \\(H+K\\). (See Exercises 33 and 34 in Section 4.1.)</p> <p>\\(\\left\\{\\mathbf{u}_1, \\mathbf{u}_2\\right\\}\\) is a basis for \\(H\\)</p> <p>\\(\\left\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\right\\}\\) is a basis for \\(K\\)</p> <p>\\(\\left\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{v}_2, \\mathbf{v}_3\\right\\}\\) is a basis for \\(H+K\\)</p> <p>Exercise 9 (4.5.6)</p> <ol> <li>find a basis for this subspace, and </li> <li>state the dimension.</li> </ol> <p>\\(\\left\\{\\left[\\begin{array}{c}3 a-c \\\\ -b-3 c \\\\ -7 a+6 b+5 c \\\\ -3 a+c\\end{array}\\right]: a, b, c\\right.\\) in \\(\\left.\\mathbb{R}\\right\\}\\)</p> <p>\\(\\left\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\right\\}\\) is linearly independent and is thus a basis for \\(H\\). Hence the dimension of \\(H\\) is 3.</p> <p>Exercise 10 (4.5.11)</p> <p>Find the dimension of the subspace spanned by the given vectors.</p> <p>\\(\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 2\\end{array}\\right],\\left[\\begin{array}{l}3 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ -1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{l}5 \\\\ 2 \\\\ 2\\end{array}\\right]\\)</p> <p>3</p> <p>Exercise 11 (4.5.14, 4.5.17)</p> <p>Determine the dimensions of \\(\\operatorname{Nul} A\\) and \\(\\operatorname{Col} A\\) for the following matricies.</p> <ol> <li>\\(A=\\left[\\begin{array}{rrrrrrr}1 &amp; 2 &amp; -4 &amp; 3 &amp; -2 &amp; 6 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; -3 &amp; 7 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rrr}1 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 3 \\\\ 0 &amp; 0 &amp; 1\\end{array}\\right]\\)</li> </ol> <ol> <li>The matrix \\(A\\) is in echelon form. There are four pivot columns, so the dimension of \\(\\operatorname{Col} A\\) is 4 . There are three columns without pivots, so the equation \\(A \\mathbf{x}=\\mathbf{0}\\) has three free variables. Thus the dimension of \\(\\operatorname{Nul} A\\) is 3 .</li> <li>The matrix \\(A\\) is in echelon form. There are three pivot columns, so the dimension of \\(\\operatorname{Col} A\\) is 3 . There are no columns without pivots, so the equation \\(A \\mathbf{x}=\\mathbf{0}\\) has only the trivial solution \\(\\mathbf{0}\\). Thus Nul \\(A=\\{\\mathbf{0}\\}\\), and the dimension of \\(\\operatorname{Nul} A\\) is 0.</li> </ol> <p>Exercise 12 (4.6.2, 4.6.4)</p> <p>Assume that the matrix \\(A\\) is row equivalent to \\(B\\). Without calculations, list \\(\\operatorname{rank} A\\) and \\(\\operatorname{dim} \\operatorname{Nul} A\\). Then find bases for \\(\\operatorname{Col} A, \\operatorname{Row} A\\), and \\(\\operatorname{Nul} A\\).</p> <ol> <li>\\(\\begin{aligned} A &amp; =\\left[\\begin{array}{rrrrr}1 &amp; 3 &amp; 4 &amp; -1 &amp; 2 \\\\ 2 &amp; 6 &amp; 6 &amp; 0 &amp; -3 \\\\ 3 &amp; 9 &amp; 3 &amp; 6 &amp; -3 \\\\ 3 &amp; 9 &amp; 0 &amp; 9 &amp; 0\\end{array}\\right], \\\\ B &amp; =\\left[\\begin{array}{rrrrr}1 &amp; 3 &amp; 4 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; -5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right],\\end{aligned}\\)</li> <li>\\(\\begin{aligned} A &amp; =\\left[\\begin{array}{rrrrrr}1 &amp; 1 &amp; -2 &amp; 0 &amp; 1 &amp; -2 \\\\ 1 &amp; 2 &amp; -3 &amp; 0 &amp; -2 &amp; -3 \\\\ 1 &amp; -1 &amp; 0 &amp; 0 &amp; 1 &amp; 6 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 &amp; -3 &amp; 0 \\\\ 1 &amp; -2 &amp; 1 &amp; 0 &amp; 2 &amp; -1\\end{array}\\right], \\\\ B &amp; =\\left[\\begin{array}{rrrrrr}1 &amp; 1 &amp; -2 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; -3 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; -13 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right]\\end{aligned}\\)</li> </ol> <ol> <li> <p>The matrix \\(B\\) is in echelon form. There are three pivot columns, so the dimension of \\(\\operatorname{Col} A\\) is 3 . There are three pivot rows, so the dimension of Row \\(A\\) is 3 . There are two columns without pivots, so the equation \\(A \\mathbf{x}=\\mathbf{0}\\) has two free variables. Thus the dimension of \\(\\operatorname{Nul} A\\) is 2 . A basis for \\(\\operatorname{Col} A\\) is the pivot columns of \\(A\\) :</p> <p>\\(\\left\\{\\left[\\begin{array}{l}1 \\\\ 2 \\\\ 3 \\\\ 3\\end{array}\\right],\\left[\\begin{array}{l}4 \\\\ 6 \\\\ 3 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}2 \\\\ -3 \\\\ -3 \\\\ 0\\end{array}\\right]\\right\\}\\)</p> <p>A basis for Row \\(A\\) is the pivot rows of \\(B:\\{(1,3,4,-1,2),(0,0,1,-1,1),(0,0,0,0,-5)\\}\\).</p> <p>A basis for \\(\\operatorname{Nul} A\\) is \\(\\left\\{\\left[\\begin{array}{r}-3 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}-3 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0\\end{array}\\right]\\right\\}\\)</p> </li> <li> <p>The matrix \\(B\\) is in echelon form. There are five pivot columns, so the dimension of \\(\\operatorname{Col} A\\) is 5 . There are five pivot rows, so the dimension of Row \\(A\\) is 5 . There is one column without a pivot, so the equation \\(A \\mathbf{x}=\\mathbf{0}\\) has one free variable. Thus the dimension of \\(\\operatorname{Nul} A\\) is 1 . A basis for \\(\\operatorname{Col} A\\) is the pivot columns of \\(A\\) :</p> <p>\\(\\left\\{\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ 2 \\\\ -1 \\\\ -2 \\\\ -2\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ -3 \\\\ 0 \\\\ 2 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}1 \\\\ -2 \\\\ 1 \\\\ -3 \\\\ 2\\end{array}\\right],\\left[\\begin{array}{r}-2 \\\\ -3 \\\\ 6 \\\\ 0 \\\\ -1\\end{array}\\right]\\right\\}\\)</p> <p>A basis for Row \\(A\\) is the pivot rows of \\(B\\) : \\(\\{(1,1,-2,0,1,-2),(0,1,-1,0,-3,-1),(0,0,1,1,-13,-1),(0,0,0,0,1,-1),(0,0,0,0,0,1)\\}\\)</p> <p>Thus a basis for \\(\\operatorname{Nul} A\\) is</p> <p>\\(\\left\\{\\left[\\begin{array}{r}-1 \\\\ -1 \\\\ -1 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right]\\right\\}\\)</p> </li> </ol> <p>Exercise 13 (4.6.35)</p> <p>[M] Let \\(A=\\left[\\begin{array}{rrrrrrr}7 &amp; -9 &amp; -4 &amp; 5 &amp; 3 &amp; -3 &amp; -7 \\\\ -4 &amp; 6 &amp; 7 &amp; -2 &amp; -6 &amp; -5 &amp; 5 \\\\ 5 &amp; -7 &amp; -6 &amp; 5 &amp; -6 &amp; 2 &amp; 8 \\\\ -3 &amp; 5 &amp; 8 &amp; -1 &amp; -7 &amp; -4 &amp; 8 \\\\ 6 &amp; -8 &amp; -5 &amp; 4 &amp; 4 &amp; 9 &amp; 3\\end{array}\\right]\\).</p> <ol> <li>Construct matrices \\(C\\) and \\(N\\) whose columns are bases for \\(\\operatorname{Col} A\\) and \\(\\operatorname{Nul} A\\), respectively, and construct a matrix \\(R\\) whose rows form a basis for Row \\(A\\).</li> <li>Construct a matrix \\(M\\) whose columns form a basis for \\(\\operatorname{Nul} A^T\\), form the matrices \\(S=\\left[\\begin{array}{ll}R^T &amp; N\\end{array}\\right]\\) and \\(T=\\left[\\begin{array}{ll}C &amp; M\\end{array}\\right]\\), and explain why \\(S\\) and \\(T\\) should be square. Verify that both \\(S\\) and \\(T\\) are invertible.</li> </ol> <ol> <li> <p>\\(C=\\left[\\begin{array}{rrrr}7 &amp; -9 &amp; 5 &amp; -3 \\\\ -4 &amp; 6 &amp; -2 &amp; -5 \\\\ 5 &amp; -7 &amp; 5 &amp; 2 \\\\ -3 &amp; 5 &amp; -1 &amp; -4 \\\\ 6 &amp; -8 &amp; 4 &amp; 9\\end{array}\\right]\\).</p> <p>\\(R=\\left[\\begin{array}{rrrrrrr}1 &amp; 0 &amp; 13 / 2 &amp; 0 &amp; 5 &amp; 0 &amp; -3 \\\\ 0 &amp; 1 &amp; 11 / 2 &amp; 0 &amp; 1 / 2 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -11 / 2 &amp; 0 &amp; 7 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1\\end{array}\\right]\\).</p> <p>\\(N=\\left[\\begin{array}{rrr}-13 / 2 &amp; -5 &amp; 3 \\\\ -11 / 2 &amp; -1 / 2 &amp; -2 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 11 / 2 &amp; -7 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1\\end{array}\\right]\\).</p> </li> <li> <p>\\(M=\\left[\\begin{array}{r}2 / 11 \\\\ 41 / 11 \\\\ 0 \\\\ -28 / 11 \\\\ 1\\end{array}\\right]\\)</p> <p>The matrix \\(S=\\left[\\begin{array}{ll}R^T &amp; N\\end{array}\\right]\\) is \\(7 \\times 7\\) because the columns of \\(R^T\\) and \\(N\\) are in \\(\\mathbb{R}^7\\) and \\(\\operatorname{dimRow} A\\) \\(+\\operatorname{dimNul} A=7\\). The matrix \\(T=\\left[\\begin{array}{ll}C &amp; M\\end{array}\\right]\\) is \\(5 \\times 5\\) because the columns of \\(C\\) and \\(M\\) are in \\(\\mathbb{R}^5\\) and \\(\\operatorname{dimCol} A+\\operatorname{dimNul} A^T=5\\). Both \\(S\\) and \\(T\\) are invertible because their columns are linearly independent. This fact will be proven in general in Theorem 3 of Section 6.1.</p> </li> </ol>"},{"location":"06_Eigenvalues/","title":"06 Eigenvalues","text":"Eigenvalues"},{"location":"06_Eigenvalues/#session-material","title":"Session Material:","text":"<p>Lay: \u200b5.1-5.3</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"06_Eigenvalues/#session-description","title":"Session Description","text":"<p>This session introduces some of the most powerful and fascinating concepts in linear algebra: eigenvalues and eigenvectors! We'll explore what these special vectors (eigenvectors) and scalars (eigenvalues) are \u2013 basically, vectors that only get scaled (not changed in direction) when a matrix transformation is applied. We'll learn how to find eigenvalues by solving the \"characteristic equation\" and then find the corresponding eigenvectors by solving a system of equations.</p> <p>Understanding the set of all eigenvectors for a specific eigenvalue (the \"eigenspace\") is also key. Finally, we'll tackle the concept of \"diagonalization\" \u2013 a super useful technique where we can transform certain matrices into simpler diagonal forms using their eigenvectors and eigenvalues. This process, often called similarity transformation, simplifies many calculations and reveals deeper properties of the matrix and the transformation it represents.</p>"},{"location":"06_Eigenvalues/#key-concepts","title":"Key Concepts","text":"<ul> <li>Eigenvalues</li> <li>Eigenvectors</li> <li>Characteristic Equation</li> <li>Eigenspaces</li> <li>Diagonalization</li> <li>Matrix Similarity</li> </ul> <p>Learning Objectives</p> <ul> <li>Define and compute eigenvalues and eigenvectors for matrices.</li> <li>Solve the characteristic equation and construct eigenspaces.</li> <li>Diagonalize matrices and explain the process of similarity transformation.</li> <li>Analyze the significance of eigenvalues and eigenvectors in matrix transformations.</li> <li>Apply diagonalization to simplify matrix computations and interpret results.</li> </ul>"},{"location":"06_Eigenvalues/#exercises","title":"Exercises","text":"<p>Exercise 1 (5.1.1)</p> <p>Is \\(\\lambda=2\\) an eigenvalue of \\(\\left[\\begin{array}{ll}3 &amp; 2 \\\\ 3 &amp; 8\\end{array}\\right]\\) ? Why or why not?</p> <p>2 is an eigenvalue of \\(A\\).</p> <p>Exercise 2 (5.1.3)</p> <p>Is \\(\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right]\\) an eigenvector of \\(\\left[\\begin{array}{ll}1 &amp; -1 \\\\ 6 &amp; -4\\end{array}\\right]\\) ? If so, find the eigenvalue.</p> <p>\\(\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right]\\) is an eigenvector of \\(A\\) with eigenvalue -2 .</p> <p>Exercise 3 (5.1.5)</p> <p>Is \\(\\left[\\begin{array}{r}3 \\\\ -2 \\\\ 1\\end{array}\\right]\\) an eigenvector of \\(\\left[\\begin{array}{rrr}-4 &amp; 3 &amp; 3 \\\\ 2 &amp; -3 &amp; -2 \\\\ -1 &amp; 0 &amp; -2\\end{array}\\right]\\) ? If so, find the eigenvalue.</p> <p>\\(\\left[\\begin{array}{r}3 \\\\ -2 \\\\ 1\\end{array}\\right]\\) is an eigenvector of \\(A\\) for the eigenvalue -5.</p> <p>Exercise 4 (5.1.7)</p> <p>Is \\(\\lambda=4\\) an eigenvalue of \\(\\left[\\begin{array}{rrr}3 &amp; 0 &amp; -1 \\\\ 2 &amp; 3 &amp; 1 \\\\ -3 &amp; 4 &amp; 5\\end{array}\\right]\\) ? If so, find one corresponding eigenvector.</p> <p>Yes, \\(\\lambda=4\\) is an eigenvalue of \\(A\\). The general solution is not requested, so to save time, simply take any nonzero value for \\(x_3\\) to produce an eigenvector. If \\(x_3=1\\), then \\(\\mathbf{x}=(-1,-1,1)\\).</p> <p>Exercise 5 (5.1.13, 5.1.16)</p> <p>Find a basis for the eigenspace corresponding to both listed eigenvalues.</p> <ol> <li>\\(A=\\left[\\begin{array}{rrr}4 &amp; 0 &amp; 1 \\\\ -2 &amp; 1 &amp; 0 \\\\ -2 &amp; 0 &amp; 1\\end{array}\\right], \\lambda=1,2,3\\)</li> <li>\\(A=\\left[\\begin{array}{rrrr}5 &amp; 0 &amp; -1 &amp; 0 \\\\ 1 &amp; 3 &amp; 0 &amp; 0 \\\\ 2 &amp; -1 &amp; 3 &amp; 0 \\\\ 4 &amp; -2 &amp; -2 &amp; 4\\end{array}\\right], \\lambda=4\\)</li> </ol> <ol> <li> <p>For \\(\\lambda=1\\) : The general solution of \\((A-I) \\mathbf{x}=\\mathbf{0}\\) is \\(x_2 \\mathbf{e}_2\\), where \\(\\mathbf{e}_2=\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right]\\), and so \\(\\mathbf{e}_2\\) provides a basis for the eigenspace.</p> <p>For \\(\\lambda=2\\) : The general solution of \\((A-2 I) \\mathbf{x}=\\mathbf{0}\\) is \\(x_3\\left[\\begin{array}{c}-1 / 2 \\\\ 1 \\\\ 1\\end{array}\\right]\\). A nice basis vector for the eigenspace is \\(\\left[\\begin{array}{r}-1 \\\\ 2 \\\\ 2\\end{array}\\right]\\).</p> <p>For \\(\\lambda=3\\) : A basis vector for the eigenspace is \\(\\left[\\begin{array}{r}-1 \\\\ 1 \\\\ 1\\end{array}\\right]\\).</p> </li> <li> <p>Basis for the eigenspace : \\(\\left\\{\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\)</p> </li> </ol> <p>Exercise 6 (5.1.40)</p> <p>[M] Use a matrix program to find the eigenvalues of the matrix. Then use the method of Example 4 with a row reduction routine to produce a basis for the eigenspace.</p> <p>\\(\\left[\\begin{array}{rrrrr}-23 &amp; 57 &amp; -9 &amp; -15 &amp; -59 \\\\ -10 &amp; 12 &amp; -10 &amp; 2 &amp; -22 \\\\ 11 &amp; 5 &amp; -3 &amp; -19 &amp; -15 \\\\ -27 &amp; 31 &amp; -27 &amp; 25 &amp; -37 \\\\ -5 &amp; -15 &amp; -5 &amp; 1 &amp; 31\\end{array}\\right]\\)</p> <p>[M] For \\(\\lambda=-14\\), basis: \\(\\left\\{\\left[\\begin{array}{r}-1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}10 \\\\ 6 \\\\ 0 \\\\ 5 \\\\ 1\\end{array}\\right]\\right\\}\\). </p> <p>For \\(\\lambda=42\\), basis: \\(\\left\\{\\left[\\begin{array}{r}0 \\\\ 1 \\\\ -2 \\\\ 5 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}-5 \\\\ -1 \\\\ -3 \\\\ 0 \\\\ 5\\end{array}\\right]\\right\\}\\).</p> <p>Exercise 7 (5.2.1, 5.2.3, 5.2.6)</p> <p>Find the characteristic polynomial and the real eigenvalues of the following matricies.</p> <ol> <li>\\(\\left[\\begin{array}{ll}2 &amp; 7 \\\\ 7 &amp; 2\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{rr}-4 &amp; 2 \\\\ 6 &amp; 7\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{rr}9 &amp; -2 \\\\ 2 &amp; 5\\end{array}\\right]\\)</li> </ol> <ol> <li>The characteristic polynomial is \\(\\operatorname{det}(A-\\lambda I)=(2-\\lambda)^2-7^2=4-4 \\lambda+\\lambda^2-49=\\lambda^2-4 \\lambda-45\\), the eigenvalues of \\(A\\) are -2 and -1.</li> <li>The characteristic polynomial is \\(\\operatorname{det}(A-\\lambda I)=(-4-\\lambda)(7-\\lambda)-(2)(6)=\\lambda^2-3 \\lambda-40\\), the eigenvalues of \\(A\\) are 8 and -5.</li> <li>The characteristic polynomial is \\(\\operatorname{det}(A-\\lambda I)=(9-\\lambda)(5-\\lambda)-(-2)(2)=\\lambda^2-14 \\lambda+49=(\\lambda-7)(\\lambda-7)\\), thus \\(A\\) has only one eigenvalue, 7, with multiplicity 2.</li> </ol> <p>Exercise 8 (5.2.10, 5.2.11)</p> <p>Following exercises require techniques from Section 3.1. Find the characteristic polynomial of each matrix, using either a cofactor expansion or the special formula for \\(3 \\times 3\\) determinants described prior to Exercises 15-18 in Section 3.1. [Note: Finding the characteristic polynomial of a \\(3 \\times 3\\) matrix is not easy to do with just row operations, because the variable \\(\\lambda\\) is involved.]</p> <ol> <li>\\(\\left[\\begin{array}{rrr}3 &amp; 1 &amp; 1 \\\\ 0 &amp; 5 &amp; 0 \\\\ -2 &amp; 0 &amp; 7\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{lll}3 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 4 \\\\ 1 &amp; 0 &amp; 4\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(-\\lambda^3+15 \\lambda^2-73 \\lambda+115\\)</li> <li>\\(-\\lambda^3+8 \\lambda^2-19 \\lambda+12\\)</li> </ol> <p>Exercise 9 (5.3.1)</p> <p>Let \\(A=P D P^{-1}\\) and compute \\(A^4\\).</p> <p>\\(P=\\left[\\begin{array}{ll}5 &amp; 7 \\\\ 2 &amp; 3\\end{array}\\right], D=\\left[\\begin{array}{ll}2 &amp; 0 \\\\ 0 &amp; 1\\end{array}\\right]\\)</p> <p>\\(A^4=\\left[\\begin{array}{ll}5 &amp; 7 \\\\ 2 &amp; 3\\end{array}\\right]\\left[\\begin{array}{cc}16 &amp; 0 \\\\ 0 &amp; 1\\end{array}\\right]\\left[\\begin{array}{rr}3 &amp; -7 \\\\ -2 &amp; 5\\end{array}\\right]=\\left[\\begin{array}{cc}226 &amp; -525 \\\\ 90 &amp; -209\\end{array}\\right]\\)</p> <p>Exercise 10 (5.3.6)</p> <p>The matrix \\(A\\) is factored in the form \\(P D P^{-1}\\). Use the Diagonalization Theorem to find the eigenvalue of \\(A\\) and a basis for the eigenspace.</p> <p>\\(\\begin{aligned} A &amp; =\\left[\\begin{array}{rrr}3 &amp; 0 &amp; 0 \\\\ -3 &amp; 4 &amp; 9 \\\\ 0 &amp; 0 &amp; 3\\end{array}\\right] \\\\ &amp; =\\left[\\begin{array}{rrr}3 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -3 \\\\ 1 &amp; 0 &amp; 0\\end{array}\\right]\\left[\\begin{array}{lll}3 &amp; 0 &amp; 0 \\\\ 0 &amp; 4 &amp; 0 \\\\ 0 &amp; 0 &amp; 3\\end{array}\\right]\\left[\\begin{array}{rrr}0 &amp; 0 &amp; 1 \\\\ -3 &amp; 1 &amp; 9 \\\\ -1 &amp; 0 &amp; 3\\end{array}\\right]\\end{aligned}\\)</p> <p>\\(\\lambda=4:\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right] ; \\lambda=3:\\left[\\begin{array}{l}3 \\\\ 0 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{c}-1 \\\\ -3 \\\\ 0\\end{array}\\right]\\)</p> <p>Exercise 11 (5.3.10-14)</p> <p>Diagonalize the matrices, if possible. The real eigenvalues for Exercises (b)-(e) are included next to the matrix.</p> <ol> <li>\\(\\left[\\begin{array}{ll}1 &amp; 3 \\\\ 4 &amp; 2\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{lll}0 &amp; 1 &amp; 1 \\\\ 2 &amp; 1 &amp; 2 \\\\ 3 &amp; 3 &amp; 2\\end{array}\\right]\\) \\(\\lambda=-1,5\\)</li> <li>\\(\\left[\\begin{array}{lll}3 &amp; 1 &amp; 1 \\\\ 1 &amp; 3 &amp; 1 \\\\ 1 &amp; 1 &amp; 3\\end{array}\\right]\\) \\(\\lambda=2,5\\)</li> <li>\\(\\left[\\begin{array}{rrr}2 &amp; 2 &amp; -1 \\\\ 1 &amp; 3 &amp; -1 \\\\ -1 &amp; -2 &amp; 2\\end{array}\\right]\\) \\(\\lambda=1,5\\)</li> <li>\\(\\left[\\begin{array}{rrr}2 &amp; 0 &amp; -2 \\\\ 1 &amp; 3 &amp; 2 \\\\ 0 &amp; 0 &amp; 3\\end{array}\\right]\\) \\(\\lambda=2,3\\)</li> </ol> <ol> <li> <p>For \\(\\lambda=-2\\): The general solution is \\(x_2\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]\\), and a nice basis vector for the eigenspace is \\(\\mathbf{v}_1=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]\\).</p> <p>For \\(\\lambda=5\\): The general solution is \\(x_2\\left[\\begin{array}{c}3 / 4 \\\\ 1\\end{array}\\right]\\), and a basis vector for the eigenspace is \\(\\mathbf{v}_2=\\left[\\begin{array}{l}3 \\\\ 4\\end{array}\\right]\\).</p> <p>\\(D=\\left[\\begin{array}{rr}-2 &amp; 0 \\\\ 0 &amp; 5\\end{array}\\right]\\)</p> </li> <li> <p>For \\(\\lambda=-1\\): The general solution is \\(x_2\\left[\\begin{array}{c}-1 \\\\ 1 \\\\ 0\\end{array}\\right]+x_3\\left[\\begin{array}{r}-1 \\\\ 0 \\\\ 1\\end{array}\\right]\\), and a nice basis for the eigenspace is \\(\\left\\{\\mathbf{v}_1, \\mathbf{v}_2\\right\\}=\\left\\{\\left[\\begin{array}{c}-1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{c}-1 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\).</p> <p>For \\(\\lambda=5\\): general solution is \\(x_3\\left[\\begin{array}{r}1 / 3 \\\\ 2 / 3 \\\\ 1\\end{array}\\right]\\), and a nice basis vector for the eigenspace is \\(\\mathbf{v}_3=\\left[\\begin{array}{l}1 \\\\ 2 \\\\ 3\\end{array}\\right]\\).</p> <p>\\(D=\\left[\\begin{array}{ccc}-1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 5\\end{array}\\right]\\)</p> </li> <li> <p>For \\(\\lambda=2\\): The general solution is \\(x_2\\left[\\begin{array}{c}-1 \\\\ 1 \\\\ 0\\end{array}\\right]+x_3\\left[\\begin{array}{r}-1 \\\\ 0 \\\\ 1\\end{array}\\right]\\), and a nice basis for the eigenspace is \\(\\left\\{\\mathbf{v}_1, \\mathbf{v}_2\\right\\}=\\left\\{\\left[\\begin{array}{c}-1 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{c}-1 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\).</p> <p>For \\(\\lambda=5\\): The general solution is \\(x_3\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right]\\), and a basis for the eigenspace is \\(\\mathbf{v}_3=\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right]\\).</p> <p>\\(D=\\left[\\begin{array}{lll}2 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 5\\end{array}\\right]\\)</p> </li> <li> <p>For \\(\\lambda=5\\): The general solution is \\(x_3\\left[\\begin{array}{r}-1 \\\\ -1 \\\\ 1\\end{array}\\right]\\), and a basis for the eigenspace is \\(\\mathbf{v}_1=\\left[\\begin{array}{r}-1 \\\\ -1 \\\\ 1\\end{array}\\right]\\).</p> <p>For \\(\\lambda=1\\): The general solution is \\(x_2\\left[\\begin{array}{r}-2 \\\\ 1 \\\\ 0\\end{array}\\right]+x_3\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 1\\end{array}\\right]\\), and a basis for the eigenspace is \\(\\left\\{\\mathbf{v}_2, \\mathbf{v}_3\\right\\}=\\left\\{\\left[\\begin{array}{r}-2 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\).</p> <p>\\(D=\\left[\\begin{array}{lll}5 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\end{array}\\right]\\)</p> </li> <li> <p>For \\(\\lambda=2\\): The general solution is \\(x_2\\left[\\begin{array}{c}-1 \\\\ 1 \\\\ 0\\end{array}\\right]\\), and a basis for the eigenspace is \\(\\mathbf{v}_1=\\left[\\begin{array}{c}-1 \\\\ 1 \\\\ 0\\end{array}\\right]\\).</p> <p>For \\(\\lambda=3\\): The general solution is \\(x_2\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right]+x_3\\left[\\begin{array}{r}-2 \\\\ 0 \\\\ 1\\end{array}\\right]\\), and a nice basis for the eigenspace is \\(\\left\\{\\mathbf{v}_2, \\mathbf{v}_3\\right\\}=\\left\\{\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{c}-2 \\\\ 0 \\\\ 1\\end{array}\\right]\\right\\}\\).</p> <p>\\(D=\\left[\\begin{array}{lll}2 &amp; 0 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 3\\end{array}\\right]\\)</p> </li> </ol> <p>Exercise 12 (5.3.35, 5.3.36)</p> <p>[M] Diagonalize the matrices. Use your matrix program's eigenvalue command to find the eigenvalues, and then compute bases for the eigenspaces as in Section 5.1.</p> <ol> <li>\\(\\left[\\begin{array}{rrrrr}13 &amp; -12 &amp; 9 &amp; -15 &amp; 9 \\\\ 6 &amp; -5 &amp; 9 &amp; -15 &amp; 9 \\\\ 6 &amp; -12 &amp; -5 &amp; 6 &amp; 9 \\\\ 6 &amp; -12 &amp; 9 &amp; -8 &amp; 9 \\\\ -6 &amp; 12 &amp; 12 &amp; -6 &amp; -2\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{rrrrr}24 &amp; -6 &amp; 2 &amp; 6 &amp; 2 \\\\ 72 &amp; 51 &amp; 9 &amp; -99 &amp; 9 \\\\ 0 &amp; -63 &amp; 15 &amp; 63 &amp; 63 \\\\ 72 &amp; 15 &amp; 9 &amp; -63 &amp; 9 \\\\ 0 &amp; 63 &amp; 21 &amp; -63 &amp; -27\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(D=\\left[\\begin{array}{ccccc}7 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 7 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 7 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -14 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; -14\\end{array}\\right]\\)</li> <li>\\(D=\\left[\\begin{array}{ccccc}24 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -48 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -48 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 36 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 36\\end{array}\\right]\\)</li> </ol>"},{"location":"07_Differential_Equations/","title":"07 Differential Equations","text":"Differential Equations"},{"location":"07_Differential_Equations/#session-material","title":"Session Material:","text":"<p>Lay: 5.7</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"07_Differential_Equations/#session-description","title":"Session Description","text":"<p>We've learned about eigenvalues and eigenvectors and how to diagonalize matrices. In this session, we'll see a powerful application of these ideas to the real world: solving systems of linear differential equations! We'll discover how the eigenvalues and eigenvectors of a matrix connected to the system give us the fundamental building blocks for the solutions.</p> <p>We'll focus on understanding how these concepts help us find the general solution to systems of equations that describe things changing over time, like population models or coupled systems. The eigenvalues will relate to the exponential growth or decay rates, and the eigenvectors will define the directions of these changes. While this is just an introduction, it highlights how abstract linear algebra tools are essential for tackling dynamic problems in science and engineering.</p>"},{"location":"07_Differential_Equations/#key-concepts","title":"Key Concepts","text":"<ul> <li>Systems of Differential Equations</li> <li>Applications of Eigenvalues</li> <li>Applications of Eigenvectors</li> <li>Linear Differential Equations</li> <li>Solution Structure</li> <li>Exponential Solutions</li> </ul> <p>Learning Objectives</p> <ul> <li>Model and solve systems of linear differential equations using matrix methods.</li> <li>Apply eigenvalues and eigenvectors to construct general solutions.</li> <li>Interpret the role of exponential solutions in dynamic systems.</li> <li>Analyze the structure of solutions for linear differential equations.</li> <li>Connect linear algebra concepts to real-world applications in science and engineering.</li> </ul>"},{"location":"07_Differential_Equations/#exercises","title":"Exercises","text":"<p>Exercise 1 (5.7.1)</p> <p>A particle moving in a planar force field has a position vector \\(\\mathbf{x}\\) that satisfies \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\). The \\(2 \\times 2\\) matrix \\(A\\) has eigenvalues 4 and 2 , with corresponding eigenvectors \\(\\mathbf{v}_1=\\left[\\begin{array}{r}-3 \\\\ 1\\end{array}\\right]\\) and \\(\\mathbf{v}_2=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]\\). Find the position of the particle at time \\(t\\), assuming that \\(\\mathbf{x}(0)=\\left[\\begin{array}{r}-6 \\\\ 1\\end{array}\\right]\\).</p> <p>\\(c_1=5 / 2, c_2=-3 / 2\\), and \\(\\mathbf{x}(t)=\\frac{5}{2}\\left[\\begin{array}{r}-3 \\\\ 1\\end{array}\\right] e^{4 t}-\\frac{3}{2}\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] e^{2 t}\\)</p> <p>Exercise 2 (5.7.2)</p> <p>Let \\(A\\) be a \\(2 \\times 2\\) matrix with eigenvalues -3 and -1 and corresponding eigenvectors \\(\\mathbf{v}_1=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]\\) and \\(\\mathbf{v}_2=\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right]\\). Let \\(\\mathbf{x}(t)\\) be the position of a particle at time \\(t\\). Solve the initial value problem \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}, \\mathbf{x}(0)=\\left[\\begin{array}{l}2 \\\\ 3\\end{array}\\right]\\).</p> <p>\\(c_1=1 / 2, c_2=5 / 2\\), and \\(\\mathbf{x}(t)=\\frac{1}{2}\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] e^{-3 t}+\\frac{5}{2}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right] e^{-t}\\)</p> <p>Exercise 3 (5.7.3-5.7.6)</p> <p>Solve the initial value problem \\(\\mathbf{x}^{\\prime}(t)=A \\mathbf{x}(t)\\) for \\(t \\geq 0\\), with \\(\\mathbf{x}(0)=(3,2)\\). Classify the nature of the origin as an attractor, repeller, or saddle point of the dynamical system described by \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\). Find the directions of greatest attraction and/or repulsion. When the origin is a saddle point, sketch typical trajectories.</p> <ol> <li>\\(A=\\left[\\begin{array}{rr}2 &amp; 3 \\\\ -1 &amp; -2\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rr}-2 &amp; -5 \\\\ 1 &amp; 4\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rr}7 &amp; -1 \\\\ 3 &amp; 3\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{ll}1 &amp; -2 \\\\ 3 &amp; -4\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(c_1=-5 / 2, c_2=9 / 2\\), and \\(\\mathbf{x}(t)=-\\frac{5}{2}\\left[\\begin{array}{r}-3 \\\\ 1\\end{array}\\right] e^t+\\frac{9}{2}\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] e^{-t}\\). Since one eigenvalue is positive and the other is negative, the origin is a saddle point of the dynamical system described by \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\). The direction of greatest attraction is the line through \\(\\mathbf{v}_2\\) and the origin. The direction of greatest repulsion is the line through \\(\\mathbf{v}_1\\) and the origin.</li> <li>\\(c_1=13 / 4, c_2=-5 / 4\\), and \\(\\mathbf{x}(t)=\\frac{13}{4}\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right] e^{3 t}-\\frac{5}{4}\\left[\\begin{array}{r}-5 \\\\ 1\\end{array}\\right] e^{-t}\\). Since one eigenvalue is positive and the other is negative, the origin is a saddle point of the dynamical system described by \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\). The direction of greatest attraction is the line through \\(\\mathbf{v}_2\\) and the origin. The direction of greatest repulsion is the line through \\(\\mathbf{v}_1\\) and the origin.</li> <li>\\(c_1=-1 / 2, c_2=7 / 2\\), and \\(\\mathbf{x}(t)=-\\frac{1}{2}\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right] e^{4 t}+\\frac{7}{2}\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right] e^{6 t}\\). Since both eigenvalues are positive, the origin is a repellor of the dynamical system described by \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\). The direction of greatest repulsion is the line through \\(\\mathbf{v}_2\\) and the origin.</li> <li>\\(c_1=-1, c_2=5\\), and \\(\\mathbf{x}(t)=-\\left[\\begin{array}{l}2 \\\\ 3\\end{array}\\right] e^{-2 t}+5\\left[\\begin{array}{l}1 \\\\ 1\\end{array}\\right] e^{-t}\\). Since both eigenvalues are negative, the origin is an attractor of the dynamical system described by \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\). The direction of greatest attraction is the line through \\(\\mathbf{v}_1\\) and the origin.</li> </ol> <p>Exercise 4 (5.7.7-5.7.8)</p> <p>Make a change of variable that decouples the equation \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\). Write the equation \\(\\mathbf{x}(t)=P \\mathbf{y}(t)\\) and show the calculation that leads to the uncoupled system \\(\\mathbf{y}^{\\prime}=D \\mathbf{y}\\), specifying \\(P\\) and \\(D\\).</p> <ol> <li>\\(A=\\left[\\begin{array}{rr}7 &amp; -1 \\\\ 3 &amp; 3\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{ll}1 &amp; -2 \\\\ 3 &amp; -4\\end{array}\\right]\\)</li> </ol> <ol> <li> <p>\\(\\left[\\begin{array}{l}y_1^{\\prime}(t) \\\\ y_2^{\\prime}(t)\\end{array}\\right]=\\left[\\begin{array}{ll}4 &amp; 0 \\\\ 0 &amp; 6\\end{array}\\right]\\left[\\begin{array}{l}y_1(t) \\\\ y_2(t)\\end{array}\\right]\\)</p> </li> <li> <p>\\(\\left[\\begin{array}{l}y_1^{\\prime}(t) \\\\ y_2^{\\prime}(t)\\end{array}\\right]=\\left[\\begin{array}{rr}-2 &amp; 0 \\\\ 0 &amp; -1\\end{array}\\right]\\left[\\begin{array}{l}y_1(t) \\\\ y_2(t)\\end{array}\\right]\\)</p> </li> </ol> <p>Exercise 5 (5.7.15-5.7.16)</p> <p>Construct the general solution of \\(\\mathbf{x}^{\\prime}=A \\mathbf{x}\\) involving complex eigenfunctions and then obtain the general real solution. Describe the shapes of typical trajectories.</p> <ol> <li>\\([\\mathbf{M}] A=\\left[\\begin{array}{rrr}-8 &amp; -12 &amp; -6 \\\\ 2 &amp; 1 &amp; 2 \\\\ 7 &amp; 12 &amp; 5\\end{array}\\right]\\)</li> <li>\\([\\mathbf{M}] A=\\left[\\begin{array}{rrr}-6 &amp; -11 &amp; 16 \\\\ 2 &amp; 5 &amp; -4 \\\\ -4 &amp; -5 &amp; 10\\end{array}\\right]\\)</li> </ol> <ol> <li>The general solution is \\(\\mathbf{x}(t)=c_1\\left[\\begin{array}{r}-4 \\\\ 1 \\\\ 4\\end{array}\\right] e^t+c_2\\left[\\begin{array}{r}-6 \\\\ 1 \\\\ 5\\end{array}\\right] e^{-t}+c_3\\left[\\begin{array}{r}-1 \\\\ 0 \\\\ 1\\end{array}\\right] e^{-2 t}\\). The origin is a saddle point. A solution with \\(c_1=0\\) is attracted to the origin while a solution with \\(c_2=c_3=0\\) is repelled.</li> <li>The general solution is \\(\\mathbf{x}(t)=c_1\\left[\\begin{array}{r}7 \\\\ -2 \\\\ 3\\end{array}\\right] e^{4 t}+c_2\\left[\\begin{array}{r}3 \\\\ -1 \\\\ 1\\end{array}\\right] e^{3 t}+c_3\\left[\\begin{array}{l}2 \\\\ 0 \\\\ 1\\end{array}\\right] e^{2 t}\\). The origin is a repellor, because all eigenvalues are positive. All trajectories tend away from the origin.</li> </ol> <p>Exercise 6 (5.7.19)</p> <p>[M] Find formulas for the voltages \\(v_1\\) and \\(v_2\\) (as functions of time \\(t\\) ) for the circuit in Example 1, assuming that \\(R_1=1 / 5\\) ohm, \\(R_2=1 / 3\\) ohm, \\(C_1=4\\) farads, \\(C_2=3\\) farads, and the initial charge on each capacitor is 4 volts.</p> <p>The general solution is \\(\\mathbf{x}(t)=c_1\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right] e^{-.5 t}+c_2\\left[\\begin{array}{r}-3 \\\\ 2\\end{array}\\right] e^{-2.5 t}\\).</p> <p>\\(\\left[\\begin{array}{l}v_1(t) \\\\ v_2(t)\\end{array}\\right]=\\mathbf{x}(t)=\\frac{5}{2}\\left[\\begin{array}{l}1 \\\\ 2\\end{array}\\right] e^{-.5 t}-\\frac{1}{2}\\left[\\begin{array}{r}-3 \\\\ 2\\end{array}\\right] e^{-2.5 t}\\)</p> <p>Exercise 7 (5.7.20)</p> <p>[M] Find formulas for the voltages \\(v_1\\) and \\(v_2\\) for the circuit in Example 1, assuming that \\(R_1=1 / 15 \\mathrm{ohm}, R_2=1 / 3 \\mathrm{ohm}\\), \\(C_1=9\\) farads, \\(C_2=2\\) farads, and the initial charge on each capacitor is 3 volts.</p> <p>The general solution is thus \\(\\mathbf{x}(t)=c_1\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right] e^{-t}+c_2\\left[\\begin{array}{r}-2 \\\\ 3\\end{array}\\right] e^{-2.5 t}\\).</p> <p>\\(\\left[\\begin{array}{l}v_1(t) \\\\ v_2(t)\\end{array}\\right]=\\mathbf{x}(t)=\\frac{5}{3}\\left[\\begin{array}{l}1 \\\\ 3\\end{array}\\right] e^{-t}-\\frac{2}{3}\\left[\\begin{array}{r}-2 \\\\ 3\\end{array}\\right] e^{-2.5 t}\\)</p>"},{"location":"08_Orthogonality_I/","title":"08 Orthogonality I","text":"Orthogonality I"},{"location":"08_Orthogonality_I/#session-material","title":"Session Material:","text":"<p>Lay: 6.1-6.3</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"08_Orthogonality_I/#session-description","title":"Session Description","text":"<p>Let's add some geometry back into our vector spaces! This session introduces the idea of the \"inner product\" (or dot product in \\(\\mathbb{R}^n\\)) as a way to measure things like the \"length\" of a vector and the \"distance\" between vectors. Most importantly, it gives us a precise way to define \"orthogonal\" (or perpendicular) vectors. We'll explore orthogonal complements \u2013 the set of all vectors truly perpendicular to a given subspace, and see how they connect fundamental spaces we've already studied.</p> <p>Building on this, we'll look at \"orthogonal sets\" of vectors, where every pair is perpendicular. These sets are really special because they're automatically linearly independent, and if they also span the space, they form an \"orthogonal basis\" \u2013 which makes finding coordinates ridiculously easy! Finally, we'll learn how to \"orthogonally project\" a vector onto a subspace. Think of this as finding the closest point in that subspace to the original vector, a super useful tool with lots of applications (related to the Best Approximation Theorem).</p>"},{"location":"08_Orthogonality_I/#key-concepts","title":"Key Concepts","text":"<ul> <li>Inner Product / Dot Product</li> <li>Vector Length and Distance</li> <li>Orthogonal Vectors, Complements, Sets, Bases, and Projection</li> <li>Best Approximation</li> </ul> <p>Learning Objectives</p> <ul> <li>Compute inner products, vector lengths, and distances in vector spaces.</li> <li>Identify and construct orthogonal and orthonormal sets and bases.</li> <li>Determine orthogonal complements and analyze their properties.</li> <li>Apply orthogonal projection to find best approximations in subspaces.</li> <li>Interpret the geometric meaning of orthogonality in linear algebra.</li> </ul>"},{"location":"08_Orthogonality_I/#exercises","title":"Exercises","text":"<p>Exercise 0</p> <p>Do this exercise first.pdf</p> <p>Exercise 1 (6.1.1)</p> <p>Compute the quantities using the vectors</p> \\[ \\mathbf{u}=\\left[\\begin{array}{r} -1 \\\\ 2 \\end{array}\\right], \\quad \\mathbf{v}=\\left[\\begin{array}{l} 4 \\\\ 6 \\end{array}\\right], \\quad \\mathbf{w}=\\left[\\begin{array}{r} 3 \\\\ -1 \\\\ -5 \\end{array}\\right], \\quad \\mathbf{x}=\\left[\\begin{array}{r} 6 \\\\ -2 \\\\ 3 \\end{array}\\right] \\] <p>\\(\\mathbf{u} \\cdot \\mathbf{u}, \\mathbf{v} \\cdot \\mathbf{u}\\), and \\(\\frac{\\mathbf{v} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}}\\)</p> <p>\\(\\mathbf{u} \\cdot \\mathbf{u}=(-1)^2+2^2=5, \\mathbf{v} \\cdot \\mathbf{u}=4(-1)+6(2)=8\\), and \\(\\frac{\\mathbf{v} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}}=\\frac{8}{5}\\).</p> <p>Exercise 2 (6.1.11)</p> <p>Find a unit vector in the direction of the given vector.</p> <p>\\(\\left[\\begin{array}{c}7 / 4 \\\\ 1 / 2 \\\\ 1\\end{array}\\right]\\)</p> <p>\\(\\frac{1}{\\sqrt{(7 / 4)^2+(1 / 2)^2+1^2}}\\left[\\begin{array}{r}7 / 4 \\\\ 1 / 2 \\\\ 1\\end{array}\\right]=\\frac{1}{\\sqrt{69 / 16}}\\left[\\begin{array}{r}7 / 4 \\\\ 1 / 2 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}7 / \\sqrt{69} \\\\ 2 / \\sqrt{69} \\\\ 4 / \\sqrt{69}\\end{array}\\right]\\)</p> <p>Exercise 3 (6.1.13)</p> <p>Find the distance between \\(\\mathbf{x}=\\left[\\begin{array}{r}10 \\\\ -3\\end{array}\\right]\\) and \\(\\mathbf{y}=\\left[\\begin{array}{l}-1 \\\\ -5\\end{array}\\right]\\).</p> <p>Since \\(\\mathbf{x}=\\left[\\begin{array}{c}10 \\\\ -3\\end{array}\\right]\\) and \\(\\mathbf{y}=\\left[\\begin{array}{l}-1 \\\\ -5\\end{array}\\right],\\|\\mathbf{x}-\\mathbf{y}\\|^2=[10-(-1)]^2+[-3-(-5)]^2=125\\) and \\(\\operatorname{dist}(\\mathbf{x}, \\mathbf{y})=\\sqrt{125}=5 \\sqrt{5}\\).</p> <p>Exercise 4 (6.1.15-6.1.18)</p> <p>Determine which pairs of vectors in the following exercises are orthogonal.</p> <ol> <li>\\(\\mathbf{a}=\\left[\\begin{array}{r}8 \\\\ -5\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{l}-2 \\\\ -3\\end{array}\\right]\\)</li> <li>\\(\\mathbf{u}=\\left[\\begin{array}{r}12 \\\\ 3 \\\\ -5\\end{array}\\right], \\mathbf{v}=\\left[\\begin{array}{r}2 \\\\ -3 \\\\ 3\\end{array}\\right]\\)</li> <li>\\(\\mathbf{u}=\\left[\\begin{array}{r}3 \\\\ 2 \\\\ -5 \\\\ 0\\end{array}\\right], \\mathbf{v}=\\left[\\begin{array}{r}-4 \\\\ 1 \\\\ -2 \\\\ 6\\end{array}\\right]\\)</li> <li>\\(\\mathbf{y}=\\left[\\begin{array}{r}-3 \\\\ 7 \\\\ 4 \\\\ 0\\end{array}\\right], \\mathbf{z}=\\left[\\begin{array}{r}1 \\\\ -8 \\\\ 15 \\\\ -7\\end{array}\\right]\\)</li> </ol> <ol> <li>Since \\(\\mathbf{a} \\cdot \\mathbf{b}=8(-2)+(-5)(-3)=-1 \\neq 0, \\mathbf{a}\\) and \\(\\mathbf{b}\\) are not orthogonal.</li> <li>Since \\(\\mathbf{u} \\cdot \\mathbf{v}=12(2)+(3)(-3)+(-5)(3)=0, \\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal.</li> <li>Since \\(\\mathbf{u} \\cdot \\mathbf{v}=3(-4)+2(1)+(-5)(-2)+0(6)=0, \\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal.</li> <li>Since \\(\\mathbf{y} \\cdot \\mathbf{z}=(-3)(1)+7(-8)+4(15)+0(-7)=1 \\neq 0, \\mathbf{y}\\) and \\(\\mathbf{z}\\) are not orthogonal.</li> </ol> <p>Exercise 5 (6.2.1)</p> <p>Determine whether the set of vectors is orthogonal.</p> <p>\\(\\left[\\begin{array}{r}-1 \\\\ 4 \\\\ -3\\end{array}\\right],\\left[\\begin{array}{l}5 \\\\ 2 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}3 \\\\ -4 \\\\ -7\\end{array}\\right]\\)</p> <p>Since \\(\\left[\\begin{array}{r}-1 \\\\ 4 \\\\ -3\\end{array}\\right] \\cdot\\left[\\begin{array}{r}3 \\\\ -4 \\\\ -7\\end{array}\\right]=2 \\neq 0\\), the set is not orthogonal.</p> <p>Exercise 6 (6.2.9)</p> <p>Show that \\(\\left\\{\\mathbf{u}_1, \\mathbf{u}_2\\right\\}\\) or \\(\\left\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\right\\}\\) is an orthogonal basis for \\(\\mathbf{R}^2\\) or \\(\\mathbf{R}^3\\), respectively. Then express \\(\\mathbf{x}\\) as a linear combination of the u's.</p> <p>\\(\\mathbf{u}_1=\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 1\\end{array}\\right], \\mathbf{u}_2=\\left[\\begin{array}{r}-1 \\\\ 4 \\\\ 1\\end{array}\\right], \\mathbf{u}_3=\\left[\\begin{array}{r}2 \\\\ 1 \\\\ -2\\end{array}\\right]\\), and \\(\\mathbf{x}=\\left[\\begin{array}{r}8 \\\\ -4 \\\\ -3\\end{array}\\right]\\)</p> <p>Since \\(\\mathbf{u}_1 \\cdot \\mathbf{u}_2=\\mathbf{u}_1 \\cdot \\mathbf{u}_3=\\mathbf{u}_2 \\cdot \\mathbf{u}_3=0,\\left\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\right\\}\\) is an orthogonal set. Since the vectors are non-zero, \\(\\mathbf{u}_1, \\mathbf{u}_2\\), and \\(\\mathbf{u}_3\\) are linearly independent by Theorem 4. Three such vectors in \\(\\mathbb{R}^3\\) automatically form a basis for \\(\\mathbb{R}^3\\). So \\(\\left\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\right\\}\\) is an orthogonal basis for \\(\\mathbb{R}^3\\). By Theorem 5,</p> \\[ \\mathbf{x}=\\frac{\\mathbf{x} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1 \\cdot \\mathbf{u}_1} \\mathbf{u}_1+\\frac{\\mathbf{x} \\cdot \\mathbf{u}_2}{\\mathbf{u}_2 \\cdot \\mathbf{u}_2} \\mathbf{u}_2+\\frac{\\mathbf{x} \\cdot \\mathbf{u}_3}{\\mathbf{u}_3 \\cdot \\mathbf{u}_3} \\mathbf{u}_3=\\frac{5}{2} \\mathbf{u}_1-\\frac{3}{2} \\mathbf{u}_2+2 \\mathbf{u}_3 \\] <p>Exercise 7 (6.2.12)</p> <p>Compute the orthogonal projection of \\(\\left[\\begin{array}{r}1 \\\\ -1\\end{array}\\right]\\) onto the line through \\(\\left[\\begin{array}{r}-1 \\\\ 3\\end{array}\\right]\\) and the origin.</p> <p>\\(\\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}=-\\frac{2}{5} \\mathbf{u}=\\left[\\begin{array}{r}2 / 5 \\\\ -6 / 5\\end{array}\\right]\\)</p> <p>Exercise 8 (6.2.21)</p> <p>Determine whether the set of vectors is orthonormal. If the set is only orthogonal, normalize the vectors to produce an orthonormal set.</p> <p>\\(\\left[\\begin{array}{l}1 / \\sqrt{10} \\\\ 3 / \\sqrt{20} \\\\ 3 / \\sqrt{20}\\end{array}\\right],\\left[\\begin{array}{c}3 / \\sqrt{10} \\\\ -1 / \\sqrt{20} \\\\ -1 / \\sqrt{20}\\end{array}\\right],\\left[\\begin{array}{c}0 \\\\ -1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right]\\)</p> <p>Let \\(\\mathbf{u}=\\left[\\begin{array}{l}1 / \\sqrt{10} \\\\ 3 / \\sqrt{20} \\\\ 3 / \\sqrt{20}\\end{array}\\right], \\mathbf{v}=\\left[\\begin{array}{r}3 / \\sqrt{10} \\\\ -1 / \\sqrt{20} \\\\ -1 / \\sqrt{20}\\end{array}\\right]\\), and \\(\\mathbf{w}=\\left[\\begin{array}{r}0 \\\\ -1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{array}\\right]\\). Since \\(\\mathbf{u} \\cdot \\mathbf{v}=\\mathbf{u} \\cdot \\mathbf{w}=\\mathbf{v} \\cdot \\mathbf{w}=0,\\{\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\}\\) is an orthogonal set. Also, \\(\\|\\mathbf{u}\\|^2=\\mathbf{u} \\cdot \\mathbf{u}=1,\\|\\mathbf{v}\\|^2=\\mathbf{v} \\cdot \\mathbf{v}=1\\), and \\(\\|\\mathbf{w}\\|^2=\\mathbf{w} \\cdot \\mathbf{w}=1\\), so \\(\\{\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\}\\) is an orthonormal set.</p> <p>Exercise 9 (6.2.35)</p> <p>[M] Show that the columns of the matrix \\(A\\) are orthogonal by making an appropriate matrix calculation. State the calculation you use.</p> <p>\\(A=\\left[\\begin{array}{rrrr}-6 &amp; -3 &amp; 6 &amp; 1 \\\\ -1 &amp; 2 &amp; 1 &amp; -6 \\\\ 3 &amp; 6 &amp; 3 &amp; -2 \\\\ 6 &amp; -3 &amp; 6 &amp; -1 \\\\ 2 &amp; -1 &amp; 2 &amp; 3 \\\\ -3 &amp; 6 &amp; 3 &amp; 2 \\\\ -2 &amp; -1 &amp; 2 &amp; -3 \\\\ 1 &amp; 2 &amp; 1 &amp; 6\\end{array}\\right]\\)</p> <p>[M] One can compute that \\(A^T A=100 I_4\\). Since the off-diagonal entries in \\(A^T A\\) are zero, the columns of \\(A\\) are orthogonal.</p> <p>Exercise 10 (6.3.1)</p> <p>You may assume that \\(\\left\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_4\\right\\}\\) is an orthogonal basis for \\(\\mathbb{R}^4\\).</p> <p>\\(\\mathbf{u}_1=\\left[\\begin{array}{r}0 \\\\ 1 \\\\ -4 \\\\ -1\\end{array}\\right], \\mathbf{u}_2=\\left[\\begin{array}{l}3 \\\\ 5 \\\\ 1 \\\\ 1\\end{array}\\right], \\mathbf{u}_3=\\left[\\begin{array}{r}1 \\\\ 0 \\\\ 1 \\\\ -4\\end{array}\\right], \\mathbf{u}_4=\\left[\\begin{array}{r}5 \\\\ -3 \\\\ -1 \\\\ 1\\end{array}\\right]\\), \\(\\mathbf{x}=\\left[\\begin{array}{r}10 \\\\ -8 \\\\ 2 \\\\ 0\\end{array}\\right]\\). </p> <p>Write \\(\\mathbf{x}\\) as the sum of two vectors, one in \\(\\operatorname{Span}\\left\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\right\\}\\) and the other in Span \\(\\left\\{\\mathbf{u}_4\\right\\}\\).</p> <p>\\(\\mathbf{x}-\\frac{\\mathbf{x} \\cdot \\mathbf{u}_4}{\\mathbf{u}_4 \\cdot \\mathbf{u}_4} \\mathbf{u}_4=\\left[\\begin{array}{r}10 \\\\ -8 \\\\ 2 \\\\ 0\\end{array}\\right]-\\left[\\begin{array}{r}10 \\\\ -6 \\\\ -2 \\\\ 2\\end{array}\\right]=\\left[\\begin{array}{r}0 \\\\ -2 \\\\ 4 \\\\ -2\\end{array}\\right]\\)</p> <p>Exercise 11 (6.3.3)</p> <p>Verify that \\(\\left\\{\\mathbf{u}_1, \\mathbf{u}_2\\right\\}\\) is an orthogonal set, and then find the orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\operatorname{Span}\\left\\{\\mathbf{u}_1, \\mathbf{u}_2\\right\\}\\).</p> <p>\\(\\mathbf{y}=\\left[\\begin{array}{r}-1 \\\\ 4 \\\\ 3\\end{array}\\right], \\mathbf{u}_1=\\left[\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right], \\mathbf{u}_2=\\left[\\begin{array}{r}-1 \\\\ 1 \\\\ 0\\end{array}\\right]\\)</p> <p>Since \\(\\mathbf{u}_1 \\cdot \\mathbf{u}_2=-1+1+0=0,\\left\\{\\mathbf{u}_1, \\mathbf{u}_2\\right\\}\\) is an orthogonal set. The orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\operatorname{Span}\\left\\{\\mathbf{u}_1, \\mathbf{u}_2\\right\\}\\) is</p> \\[ \\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1 \\cdot \\mathbf{u}_1} \\mathbf{u}_1+\\frac{\\mathbf{y} \\cdot \\mathbf{u}_2}{\\mathbf{u}_2 \\cdot \\mathbf{u}_2} \\mathbf{u}_2=\\frac{3}{2} \\mathbf{u}_1+\\frac{5}{2} \\mathbf{u}_2=\\frac{3}{2}\\left[\\begin{array}{l} 1 \\\\ 1 \\\\ 0 \\end{array}\\right]+\\frac{5}{2}\\left[\\begin{array}{r} -1 \\\\ 1 \\\\ 0 \\end{array}\\right]=\\left[\\begin{array}{r} -1 \\\\ 4 \\\\ 0 \\end{array}\\right] \\] <p>Exercise 12 (6.3.7)</p> <p>Let \\(W\\) be the subspace spanned by the \\(\\mathbf{u}\\) 's, and write \\(y\\) as the sum of a vector in \\(W\\) and a vector orthogonal to \\(W\\).</p> <p>\\(\\mathbf{y}=\\left[\\begin{array}{l}1 \\\\ 3 \\\\ 5\\end{array}\\right], \\mathbf{u}_1=\\left[\\begin{array}{r}1 \\\\ 3 \\\\ -2\\end{array}\\right], \\mathbf{u}_2=\\left[\\begin{array}{l}5 \\\\ 1 \\\\ 4\\end{array}\\right]\\)</p> <p>\\(\\hat{\\mathbf{y}}=\\frac{\\mathbf{y} \\cdot \\mathbf{u}_1}{\\mathbf{u}_1 \\cdot \\mathbf{u}_1} \\mathbf{u}_1+\\frac{\\mathbf{y} \\cdot \\mathbf{u}_2}{\\mathbf{u}_2 \\cdot \\mathbf{u}_2} \\mathbf{u}_2=0 \\mathbf{u}_1+\\frac{2}{3} \\mathbf{u}_2=\\left[\\begin{array}{r}10 / 3 \\\\ 2 / 3 \\\\ 8 / 3\\end{array}\\right], \\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}=\\left[\\begin{array}{r}-7 / 3 \\\\ 7 / 3 \\\\ 7 / 3\\end{array}\\right]\\)</p> <p>Exercise 13 (6.3.13)</p> <p>Find the best approximation to \\(\\mathbf{z}\\) by vectors of the form \\(c_1 \\mathbf{v}_1+c_2 \\mathbf{v}_2\\).</p> <p>\\(\\mathbf{z}=\\left[\\begin{array}{r}3 \\\\ -7 \\\\ 2 \\\\ 3\\end{array}\\right], \\mathbf{v}_1=\\left[\\begin{array}{r}2 \\\\ -1 \\\\ -3 \\\\ 1\\end{array}\\right], \\mathbf{v}_2=\\left[\\begin{array}{r}1 \\\\ 1 \\\\ 0 \\\\ -1\\end{array}\\right]\\)</p> <p>\\(\\hat{\\mathbf{z}}=\\frac{\\mathbf{z} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1} \\mathbf{v}_1+\\frac{\\mathbf{z} \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2} \\mathbf{v}_2=\\frac{2}{3} \\mathbf{v}_1-\\frac{7}{3} \\mathbf{v}_2=\\left[\\begin{array}{c}-1 \\\\ -3 \\\\ -2 \\\\ 3\\end{array}\\right]\\)</p> <p>Exercise 14 (6.3.17)</p> <p>Let \\(\\mathbf{y}=\\left[\\begin{array}{l}4 \\\\ 8 \\\\ 1\\end{array}\\right], \\quad \\mathbf{u}_1=\\left[\\begin{array}{l}2 / 3 \\\\ 1 / 3 \\\\ 2 / 3\\end{array}\\right], \\quad \\mathbf{u}_2=\\left[\\begin{array}{r}-2 / 3 \\\\ 2 / 3 \\\\ 1 / 3\\end{array}\\right], \\quad\\) and \\(W=\\operatorname{Span}\\left\\{\\mathbf{u}_1, \\mathbf{u}_2\\right\\}\\).</p> <ol> <li>Let \\(U=\\left[\\begin{array}{ll}\\mathbf{u}_1 &amp; \\mathbf{u}_2\\end{array}\\right]\\). Compute \\(U^T U\\) and \\(U U^T\\).</li> <li>Compute \\(\\operatorname{proj}_W \\mathbf{y}\\) and \\(\\left(U U^T\\right) \\mathbf{y}\\).</li> </ol> <ol> <li>\\(U^T U=\\left[\\begin{array}{ll}1 &amp; 0 \\\\ 0 &amp; 1\\end{array}\\right], U U^T=\\left[\\begin{array}{rrr}8 / 9 &amp; -2 / 9 &amp; 2 / 9 \\\\ -2 / 9 &amp; 5 / 9 &amp; 4 / 9 \\\\ 2 / 9 &amp; 4 / 9 &amp; 5 / 9\\end{array}\\right]\\)</li> <li>Since \\(U^T U=I_2\\), the columns of \\(U\\) form an orthonormal basis for \\(W\\), and by Theorem 10 \\(\\operatorname{proj}_W \\mathbf{y}=U U^T \\mathbf{y}=\\left[\\begin{array}{rrr}8 / 9 &amp; -2 / 9 &amp; 2 / 9 \\\\ -2 / 9 &amp; 5 / 9 &amp; 4 / 9 \\\\ 2 / 9 &amp; 4 / 9 &amp; 5 / 9\\end{array}\\right]\\left[\\begin{array}{l}4 \\\\ 8 \\\\ 1\\end{array}\\right]=\\left[\\begin{array}{l}2 \\\\ 4 \\\\ 5\\end{array}\\right]\\).</li> </ol>"},{"location":"09_Orthogonality_II/","title":"09 Orthogonality II","text":"Orthogonality II"},{"location":"09_Orthogonality_II/#session-material","title":"Session Material:","text":"<p>Lay: \u200b\u200b\u200b6.4-6.6</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"09_Orthogonality_II/#session-description","title":"Session Description","text":"<p>Building on our understanding of orthogonal vectors and bases, this session focuses on powerful techniques and applications related to orthogonality. First, we'll learn the \"Gram-Schmidt process\" \u2013 a step-by-step method to turn any basis for a subspace into an orthogonal (or orthonormal) one. This is a fundamental algorithm! Related to Gram-Schmidt is the \"QR factorization,\" a way to break down certain matrices into a product of a matrix with orthonormal columns and an upper triangular matrix.</p> <p>Then, we'll tackle a super common problem in real-world data: what do you do when a linear system \\(A\\mathbf{x}=\\mathbf{b}\\) has no exact solution? We'll introduce \"least-squares\" solutions \u2013 the 'best possible' approximate solutions that minimize the error. Geometrically, finding the least-squares solution involves projecting the vector \\(\\mathbf{b}\\) onto the column space of matrix \\(A\\). We'll see how these least-squares ideas are applied, especially in fitting models to data (like finding the 'best' line through a set of points).</p>"},{"location":"09_Orthogonality_II/#key-concepts","title":"Key Concepts","text":"<ul> <li>Gram-Schmidt Process</li> <li>Orthogonalization</li> <li>Least Squares</li> <li>Inconsistent Systems</li> <li>Projections</li> <li>Data Fitting</li> </ul> <p>Learning Objectives</p> <ul> <li>Apply the Gram-Schmidt process to construct orthogonal and orthonormal bases.</li> <li>Solve least-squares problems and interpret best-fit solutions.</li> <li>Analyze projections and their role in data fitting and inconsistent systems.</li> <li>Connect orthogonality concepts to practical applications in modeling and data analysis.</li> </ul>"},{"location":"09_Orthogonality_II/#exercises","title":"Exercises","text":"<p>Exercise 1 (6.4.1, 6.4.6)</p> <p>In the following exercises, the given set is a basis for a subspace \\(W\\). Use the Gram-Schmidt process to produce an orthogonal basis for \\(W\\).</p> <ol> <li>\\(\\left[\\begin{array}{r}3 \\\\ 0 \\\\ -1\\end{array}\\right],\\left[\\begin{array}{r}8 \\\\ 5 \\\\ -6\\end{array}\\right]\\)</li> <li>\\(\\left[\\begin{array}{r}3 \\\\ -1 \\\\ 2 \\\\ -1\\end{array}\\right],\\left[\\begin{array}{r}-5 \\\\ 9 \\\\ -9 \\\\ 3\\end{array}\\right]\\)</li> </ol> <ol> <li>\\(\\left\\{\\left[\\begin{array}{r}3 \\\\ 0 \\\\ -1\\end{array}\\right],\\left[\\begin{array}{r}-1 \\\\ 5 \\\\ -3\\end{array}\\right]\\right\\}\\)</li> <li>\\(\\left\\{\\left[\\begin{array}{r}3 \\\\ -1 \\\\ 2 \\\\ -1\\end{array}\\right],\\left[\\begin{array}{r}4 \\\\ 6 \\\\ -3 \\\\ 0\\end{array}\\right]\\right\\}\\)</li> </ol> <p>Exercise 2 (6.4.12)</p> <p>Find an orthogonal basis for the column space of the following matrix:</p> <p>\\(\\left[\\begin{array}{rrr}1 &amp; 3 &amp; 5 \\\\ -1 &amp; -3 &amp; 1 \\\\ 0 &amp; 2 &amp; 3 \\\\ 1 &amp; 5 &amp; 2 \\\\ 1 &amp; 5 &amp; 8\\end{array}\\right]\\)</p> <p>\\(\\left\\{\\left[\\begin{array}{r}1 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}-1 \\\\ 1 \\\\ 2 \\\\ 1 \\\\ 1\\end{array}\\right],\\left[\\begin{array}{r}3 \\\\ 3 \\\\ 0 \\\\ -3 \\\\ 3\\end{array}\\right]\\right\\}\\)</p> <p>Exercise 3 (6.4.24)</p> <p>[M] Use the Gram-Schmidt process as in Example 2 to produce an orthogonal basis for the column space of</p> \\[ A=\\left[\\begin{array}{rrrr} -10 &amp; 13 &amp; 7 &amp; -11 \\\\ 2 &amp; 1 &amp; -5 &amp; 3 \\\\ -6 &amp; 3 &amp; 13 &amp; -3 \\\\ 16 &amp; -16 &amp; -2 &amp; 5 \\\\ 2 &amp; 1 &amp; -5 &amp; -7 \\end{array}\\right] \\] <p>\\(\\left\\{\\left[\\begin{array}{r}-10 \\\\ 2 \\\\ -6 \\\\ 16 \\\\ 2\\end{array}\\right],\\left[\\begin{array}{r}3 \\\\ 3 \\\\ -3 \\\\ 0 \\\\ 3\\end{array}\\right],\\left[\\begin{array}{l}6 \\\\ 0 \\\\ 6 \\\\ 6 \\\\ 0\\end{array}\\right],\\left[\\begin{array}{r}0 \\\\ 5 \\\\ 0 \\\\ 0 \\\\ -5\\end{array}\\right]\\right\\}\\)</p> <p>Exercise 4 (6.5.1, 6.5.3) </p> <p>Find a least-squares solution of \\(A \\mathbf{x}=\\mathbf{b}\\) by (a) constructing the normal equations for \\(\\hat{\\mathbf{x}}\\) and (b) solving for \\(\\hat{\\mathbf{x}}\\).</p> <ol> <li>\\(A=\\left[\\begin{array}{rr}-1 &amp; 2 \\\\ 2 &amp; -3 \\\\ -1 &amp; 3\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{l}4 \\\\ 1 \\\\ 2\\end{array}\\right]\\)</li> <li>\\(A=\\left[\\begin{array}{rr}1 &amp; -2 \\\\ -1 &amp; 2 \\\\ 0 &amp; 3 \\\\ 2 &amp; 5\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{r}3 \\\\ 1 \\\\ -4 \\\\ 2\\end{array}\\right]\\)</li> </ol> <ol> <li> <p>(a) The normal equations are \\(\\left(A^T A\\right) \\mathbf{x}=A^T \\mathbf{b}:\\left[\\begin{array}{rr}6 &amp; -11 \\\\ -11 &amp; 22\\end{array}\\right]\\left[\\begin{array}{c}x_1 \\\\ x_2\\end{array}\\right]=\\left[\\begin{array}{c}-4 \\\\ 11\\end{array}\\right]\\).</p> <p>(b) \\(\\frac{1}{11}\\left[\\begin{array}{l}33 \\\\ 22\\end{array}\\right]=\\left[\\begin{array}{l}3 \\\\ 2\\end{array}\\right]\\)</p> </li> <li> <p>(a) The normal equations are \\(\\left(A^T A\\right) \\mathbf{x}=A^T \\mathbf{b}:\\left[\\begin{array}{rr}6 &amp; 6 \\\\ 6 &amp; 42\\end{array}\\right]\\left[\\begin{array}{l}x_1 \\\\ x_2\\end{array}\\right]=\\left[\\begin{array}{r}6 \\\\ -6\\end{array}\\right]\\)</p> <p>(b) \\(\\frac{1}{216}\\left[\\begin{array}{l}288 \\\\ -72\\end{array}\\right]=\\left[\\begin{array}{r}4 / 3 \\\\ -1 / 3\\end{array}\\right]\\)</p> </li> </ol> <p>Exercise 5 (6.5.5)</p> <p>Describe all least-squares solutions of the equation \\(A \\mathbf{x}=\\mathbf{b}\\).</p> <p>\\(A=\\left[\\begin{array}{lll}1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{l}1 \\\\ 3 \\\\ 8 \\\\ 2\\end{array}\\right]\\)</p> <p>all vectors of the form \\(\\hat{\\mathbf{x}}=\\left[\\begin{array}{r}5 \\\\ -3 \\\\ 0\\end{array}\\right]+x_3\\left[\\begin{array}{r}-1 \\\\ 1 \\\\ 1\\end{array}\\right]\\) are the least-squares solutions of \\(A \\mathbf{x}=\\mathbf{b}\\).</p> <p>Exercise 6 (6.5.7)</p> <p>Compute the least-squares error associated with the leastsquares solution found in Exercise 4b.</p> <p>The least squares error is \\(\\|A \\hat{\\mathbf{x}}-\\mathbf{b}\\|=\\sqrt{20}=2 \\sqrt{5}\\).</p> <p>Exercise 7 (6.5.12)</p> <p>Find </p> <ol> <li>The orthogonal projection of \\(\\mathbf{b}\\) onto \\(\\mathrm{Col} A\\) and </li> <li>A least-squares solution of \\(A \\mathbf{x}=\\mathbf{b}\\).</li> </ol> <p>\\(A=\\left[\\begin{array}{rrr}1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 1 \\\\ -1 &amp; 1 &amp; -1\\end{array}\\right], \\mathbf{b}=\\left[\\begin{array}{l}2 \\\\ 5 \\\\ 6 \\\\ 6\\end{array}\\right]\\)</p> <ol> <li> <p>\\(\\frac{1}{3}\\left[\\begin{array}{c}1 \\\\ 1 \\\\ 0 \\\\ -1\\end{array}\\right]+\\frac{14}{3}\\left[\\begin{array}{l}1 \\\\ 0 \\\\ 1 \\\\ 1\\end{array}\\right]-\\frac{5}{3}\\left[\\begin{array}{c}0 \\\\ -1 \\\\ 1 \\\\ -1\\end{array}\\right]=\\left[\\begin{array}{l}5 \\\\ 2 \\\\ 3 \\\\ 6\\end{array}\\right]\\)</p> </li> <li> <p>\\(\\hat{\\mathbf{x}}=\\left[\\begin{array}{r}1 / 3 \\\\ 14 / 3 \\\\ -5 / 3\\end{array}\\right]\\)</p> </li> </ol> <p>Exercise 8 (6.5.25)</p> <p>Describe all least-squares solutions of the system</p> \\[ \\begin{aligned} &amp; x+y=2 \\\\ &amp; x+y=4 \\end{aligned} \\] <p>The normal equations are \\(\\left[\\begin{array}{ll}2 &amp; 2 \\\\ 2 &amp; 2\\end{array}\\right]\\left[\\begin{array}{l}x \\\\ y\\end{array}\\right]=\\left[\\begin{array}{l}6 \\\\ 6\\end{array}\\right]\\), whose solution is the set of all \\((x, y)\\) such that \\(x+y=\\) 3. The solutions correspond to the points on the line midway between the lines \\(x+y=2\\) and \\(x+y=\\) 4.</p> <p>Exercise 9 (6.6.3-6.6.4)</p> <p>Find the equation \\(y=\\beta_0+\\beta_1 x\\) of the leastsquares line that best fits the given data points.</p> <ol> <li>\\((-1,0),(0,1),(1,2),(2,4)\\)</li> <li>\\((2,3),(3,2),(5,1),(6,0)\\)</li> </ol> <ol> <li>The least-squares line \\(y=\\beta_0+\\beta_1 x\\) is thus \\(y=1.1+1.3 x\\).</li> <li>The least-squares line \\(y=\\beta_0+\\beta_1 x\\) is thus \\(y=4.3-.7 x\\).</li> </ol> <p>Exercise 10 (6.6.7)</p> <p>A certain experiment produces the data \\((1,1.8),(2,2.7)\\), \\((3,3.4),(4,3.8),(5,3.9)\\). Describe the model that produces a least-squares fit of these points by a function of the form \\(y=\\beta_1 x+\\beta_2 x^2\\) Such a function might arise, for example, as the revenue from the sale of \\(x\\) units of a product, when the amount offered for sale affects the price to be set for the product.</p> <ol> <li>Give the design matrix, the observation vector, and the unknown parameter vector.</li> <li>[M] Find the associated least-squares curve for the data.</li> </ol> <ol> <li> <p>\\(\\mathbf{y}=X \\beta+\\epsilon\\), where \\(X=\\left[\\begin{array}{rr}1 &amp; 1 \\\\ 2 &amp; 4 \\\\ 3 &amp; 9 \\\\ 4 &amp; 16 \\\\ 5 &amp; 25\\end{array}\\right], \\mathbf{y}=\\left[\\begin{array}{l}1.8 \\\\ 2.7 \\\\ 3.4 \\\\ 3.8 \\\\ 3.9\\end{array}\\right], {\\beta}=\\left[\\begin{array}{l}\\beta_1 \\\\ \\beta_2\\end{array}\\right]\\), and \\(\\epsilon=\\left[\\begin{array}{l}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5\\end{array}\\right]\\)</p> </li> <li> <p>[M] One computes that (to two decimal places) \\(\\hat{{\\beta}}=\\left[\\begin{array}{c}1.76 \\\\ -.20\\end{array}\\right]\\), so the desired least-squares equation is \\(y=1.76 x-.20 x^2\\).</p> </li> </ol> <p>Exercise 11 (6.6.13)</p> <p>[M] To measure the takeoff performance of an airplane, the horizontal position of the plane was measured every second, from \\(t=0\\) to \\(t=12\\). The positions (in feet) were: \\(0,8.8\\), \\(29.9,62.0,104.7,159.1,222.0,294.5,380.4,471.1,571.7\\), 686.8 , and 809.2.</p> <ol> <li>Find the least-squares cubic curve \\(y=\\beta_0+\\beta_1 t+\\) \\(\\beta_2 t^2+\\beta_3 t^3\\) for these data.</li> <li>Use the result of part (a) to estimate the velocity of the plane when \\(t=4.5\\) seconds.</li> </ol> <ol> <li>The desired least-squares polynomial is \\(y(t)=-.8558+4.7025 t+5.5554 t^2-.0274 t^3\\).</li> <li>The velocity \\(v(t)\\) is the derivative of the position function \\(y(t)\\), so \\(v(t)=4.7025+11.1108 t-.0822 t^2\\), and \\(v(4.5)=53.0 \\mathrm{ft} / \\mathrm{sec}\\).</li> </ol>"},{"location":"10_Symmetric_Matrices_SVD_and_PCA/","title":"10 Symmetric Matrices, SVD, and PCA","text":"Symmetric Matrices, SVD, and PCA"},{"location":"10_Symmetric_Matrices_SVD_and_PCA/#session-material","title":"Session Material:","text":"<p>Lay: \u200b\u200b\u200b\u200b7.1+7.4-7.5  </p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"10_Symmetric_Matrices_SVD_and_PCA/#session-description","title":"Session Description","text":"<p>This session dives into some advanced and incredibly useful matrix factorizations. We start by looking at special matrices \u2013 the \"symmetric matrices\" (where A equals its transpose) \u2013 and discover that they have remarkable properties, particularly that they can always be \"orthogonally diagonalized\" (meaning we can find an orthonormal basis of eigenvectors). This fundamental result is captured by the \"Spectral Theorem.\"</p> <p>Then, we move to a factorization that applies to any matrix, not just square or symmetric ones: the \"Singular Value Decomposition\" (SVD). We'll learn about \"singular values\" and \"singular vectors\" and how they provide a powerful way to understand the structure and geometric action of any linear transformation. Finally, we'll explore some of the many real-world \"applications of SVD,\" seeing how this decomposition is essential in areas like data analysis, image processing (for compression), and even in finding the best approximate solutions to systems using the \"pseudoinverse.\"</p>"},{"location":"10_Symmetric_Matrices_SVD_and_PCA/#key-concepts","title":"Key Concepts","text":"<ul> <li>Symmetric Matrices</li> <li>Orthogonal Diagonalization</li> <li>Spectral Theorem</li> <li>Singular Value Decomposition (SVD)</li> <li>Applications of SVD</li> <li>Pseudoinverse</li> </ul> <p>Learning Objectives</p> <ul> <li>Identify and analyze properties of symmetric matrices and their diagonalization.</li> <li>Apply the Spectral Theorem to orthogonally diagonalize matrices.</li> <li>Perform and interpret Singular Value Decomposition (SVD) for general matrices.</li> <li>Relate singular values and vectors to the structure of linear transformations.</li> <li>Apply SVD and the pseudoinverse to solve problems in data analysis and engineering.</li> </ul>"},{"location":"10_Symmetric_Matrices_SVD_and_PCA/#exercises","title":"Exercises","text":"<p>ALL SVD problems from the Exam Cases (2018-s2020)</p> <p>If you do not finish in class, work on them at home</p>"},{"location":"11_SVD_and_PCA/","title":"11 SVD and PCA","text":"SVD and PCA"},{"location":"11_SVD_and_PCA/#session-material","title":"Session Material:","text":"<p>Lay: </p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"11_SVD_and_PCA/#session-description","title":"Session Description","text":""},{"location":"11_SVD_and_PCA/#key-concepts","title":"Key Concepts","text":"<p>Learning Objectives</p>"},{"location":"11_SVD_and_PCA/#exercises","title":"Exercises","text":""},{"location":"12_Recap_and_Conclusion/","title":"12 Recap and Conclusion","text":"Recap and Conclusion"},{"location":"12_Recap_and_Conclusion/#session-material","title":"Session Material:","text":"<p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"12_Recap_and_Conclusion/#session-description","title":"Session Description","text":"<p>Alright, this is our final scheduled session! We've covered a massive amount of material in applied linear algebra, building from solving basic systems all the way through vector spaces, transformations, eigenvalues, orthogonality, and powerful factorizations like the SVD.</p> <p>This session is purely dedicated to you. It's a recap and Q&amp;A session. Bring questions you have about any topic we've touched on throughout the course. We'll work through them together, and I'll do my best to clarify any concepts or problems that are still unclear. This is your chance to solidify your understanding and ensure you're ready for the final exam.</p>"},{"location":"12_Recap_and_Conclusion/#key-concepts","title":"Key Concepts","text":"<ul> <li>Recap of Key Concepts</li> <li>Q&amp;A Session</li> <li>Final Exam Preparation</li> <li>May be used as a buffer session for any topics that need more time</li> </ul>"},{"location":"12_Recap_and_Conclusion/#exercises","title":"Exercises","text":"<p>Do old exam cases</p>"},{"location":"Sessions/","title":"Sessions","text":"<p>Click on a session to the left (or below) to access a plan of a specific session and additional resources for that session. All sessions are scheduled from 8:20 to 11:50 in room C03.12, except the first session, which is from 8:20 to 14:20. The sessions are as follows:</p> Session Date Topic 01 4 Aug 08:20 \u2013 14:20 Introduction to Linear Algebra 02 5 Aug 08:20 \u2013 11:50 Independence and Linear Transformations 03 6 Aug 08:20 \u2013 11:50 Matrix Algebra 04 7 Aug 08:20 \u2013 11:50 Determinants 05 8 Aug 08:20 \u2013 11:50 Vector Spaces 06 11 Aug 08:20 \u2013 11:50 Eigenvalues 07 12 Aug 08:20 \u2013 11:50 Differential Equations 08 13 Aug 08:20 \u2013 11:50 Orthogonality 1 09 14 Aug 08:20 \u2013 11:50 Orthogonality 2 10 15 Aug 08:20 \u2013 11:50 Symmetric Matrices, SVD and PCA 11 18 Aug 08:20 \u2013 11:50 SVD and PCA 12 19 Aug 08:20 \u2013 11:50 Recap and Conclusion"},{"location":"blog/","title":"Blog","text":""},{"location":"pages/exam/","title":"Exam","text":"Exam"},{"location":"pages/exam/#exam-prerequisites","title":"Exam prerequisites:","text":"<p>None</p>"},{"location":"pages/exam/#exam-type","title":"Exam type","text":"<p>The exam has two parts:</p> <ul> <li>The first part is a Flowlock exam in Wiseflow.</li> <li>The second part is a Wiseflow exam without Flowlock. The second part must be completed in the Jupyter Notebook environment and the answers must be submitted in Wiseflow.</li> </ul> <p>Part 1 has a duration of 3 hours and part 2 has a duration of 1 hour. The exam has a total duration of 4 hours.  The student will not be able to access the second part before the first part is concluded. Part 1 weighs 75% and Part 2 weighs 25% in the final grade.</p>"},{"location":"pages/exam/#tools-allowed","title":"Tools allowed","text":"<p>In the first part the students are allowed to use any notes, books, and/or other written/printed material and will have access to PDF files on their laptop.</p> <p>The students may bring their own calculator.</p> <p>In the second part all supplementary materials and aids are allowed, e.g., using a computer as a reference work.</p> <p>It is not allowed, however, to use AI-tools such as Copilot, ChatGPT, Bing, etc. Communication of any sort is not allowed during the exam and will lead to expulsion of all involved parties from the exam.</p>"},{"location":"pages/exam/#re-exam","title":"Re-exam","text":"<p>Re-exams may be oral.</p>"},{"location":"pages/faq/","title":"FAQ","text":"FAQ"},{"location":"pages/faq/#general-information","title":"General information","text":"Is there a FAQ for the course? <p>Yes, you are looking at it! This FAQ is designed to answer some of the most common questions about the course. If you have a question that is not answered here, please feel free to contact the course responsible, Richard Brooks</p> What is the course about? <p>The course covers an introduction to linear algebra and its applications, particularly its relation to machine learning and computer graphics. In depth description can be found in the course description or by going through the description of each session in the Sessions menu.</p> How is the course related to the study program? <p>The course mostly relates to the study program by providing a foundation for understanding and applying linear algebra in the context of engineering, especially in the field of data science and machine learning as well as computer graphics.</p> What are the prerequisites for the course? <p>It is important that you recap some of your high-school math. Most importantly:</p> <ul> <li>Linear equations</li> <li>Systems of linear equations</li> <li>Vectors and vector operations</li> <li>Differential equations</li> </ul> Who should take this course? <p>The course is intended for students who are interested in learning about linear algebra and how to apply it in the context of engineering. It is very useful for students who are interested in data science and machine learning.  The course is mandatory for some Master's programs.</p> Is attendance mandatory? <p>Attendance is not mandatory, but it is highly recommended. The course is designed to be interactive and hands-on, so attending the lectures will help you understand the material better. If you are unable to attend a lecture, please make sure to catch up on the material covered in class.</p>"},{"location":"pages/faq/#who-to-contact","title":"Who to contact?","text":"Who should I contact if I have questions about the course content? <p>You can contact the course responsible, Richard Brooks, if you have questions about the course content.</p> Who should I contact if I have questions about the exam? <p>You should always contact the Study Service if you have questions about the exam.</p> Who should I contact if I have questions about the schedule? <p>You can contact our scheduler if you have questions about the schedule.</p> Who should I contact if I have scheduling conflicts? <p>You can contact our scheduler if you have questions about scheduling conflicts.</p> Who should I contact if I want to know whether this course is mandatory for a Master's program or if it satisfies a specific requirement? <p>You can contact the Study Councillor.</p>"},{"location":"pages/faq/#exam-and-assessment","title":"Exam and assessment","text":"What type of exam will conclude the course <p>Please see the \"Exam\" section in the menu to the left for detailed information about the exam.</p> When will the exam and re-exam be held? <p>The exam is held August 22 and the re-exam is August 29. Feel free to contact the Study Service for more information.</p> What is the grading scale for the course? <p>The grading scale for the course is the 7-point grading scale.</p> How is the final grade calculated? <p>The final grade is calculated based on the exam. The exam has two parts: a Flowlock exam in Wiseflow and a Wiseflow exam without Flowlock. Part 1 weighs 75% and Part 2 weighs 25% in the final grade. Please see the \"Exam\" section in the menu to the left for more information.</p> What is the re-exam procedure? <p>Re-exams may be oral. Please see the \"Exam\" section in the menu to the left for more information.</p> What happens if I fail the exam? <p>If you fail the exam, you will have the opportunity to take a re-exam. Please see the \"Exam\" section in the menu to the left for more information.</p> What happens if I fail the re-exam? <p>If you fail the re-exam, you will have to wait until the course is held again to retake the exam. In special cases, we may be able to offer you an oral re-exam. Please see the \"Exam\" section in the menu to the left for more information.</p> How many percentage points do I need to pass the exam? <p>The exam is graded on a 7-point grading scale. To pass the exam, you need a grade of 02 or higher. In order to obtain a 02, you need to score at least 50% of the total points on the exam. This score may vary from year to year, but is never higher than 50%.</p>"},{"location":"pages/faq/#resources","title":"Resources","text":"Is the course book mandatory? <p>The course book is not mandatory, but I do recommend finding some resources to help you understand the material. The course book is a good resource, but there are many other resources available online. Please also see the Online Resources section in the menu to the left.</p> Is Python mandatory? <p>Python is mandatory for the course. You will need to use Python to complete the assignments and the exam. If you are not familiar with Python, I recommend finding some resources to help you learn the basics.</p> Is Jupyter Notebook mandatory? <p>Jupyter Notebook is mandatory for the course. You will need to use Jupyter Notebook to complete the assignments and the exam. You can install a plugin in VSCode to run Jupyter Notebooks.</p> Is the course material available online? <p>Yes, the course material is available online. You can find all material by navigating the menu to the left.</p> Is there a recommended study plan? <p>Yes, I recommend following the study plan outlined in the course material. You can find the study plan in the course material by navigating the menu to the left.</p> Where do I find material such as old exam cases, solutions, and other resources? <p>You can find all material by navigating the menu to the left, see \"General Resources ALI\".</p> Are there any additional resources available? <p>Yes, there are many additional resources available online. Please also see the Online Resources section in the menu to the left.</p> What is the Wiseflow code? <p>The Wiseflow code for the course is 0000. However, at the exam you will be given specific codes.</p> What does [M] mean in the exercises? <p>The [M] means that you are supposed to solve the exercise using Matlab. Disregard this and use Python.</p>"},{"location":"pages/online_resources/","title":"Online Resources","text":"Recommended Online Resources <p>Below are some recommended online resources that can help you with the course material. These resources are not mandatory, but they can be very helpful in understanding the concepts covered in the course.</p>"},{"location":"pages/online_resources/#gilbert-strang-mit-1806-linear-algebra","title":"Gilbert Strang: MIT 18.06 Linear Algebra","text":"<p>This is the OG of linear algebra courses and the one I used when I had to prepare for this course when it ran first time back in 2014. The course is available on MIT OpenCourseWare. It includes lecture notes, assignments, and exams. The course is taught by Gilbert Strang, a renowned mathematician and professor at MIT. The lectures are clear and concise, and the course covers a wide range of topics in linear algebra. The course is also available on YouTube that inlcudes an interview with Prof. Strang.</p>"},{"location":"pages/online_resources/#3blue1brown","title":"3blue1brown","text":"<p>3blue1brown is a YouTube channel that provides visual explanations of various mathematical concepts, including linear algebra. The videos are engaging and can help you understand the material better. The channel is run by Grant Sanderson, who has a knack for making complex topics accessible and enjoyable. The videos are well-produced and often include animations that illustrate the concepts being discussed. The Essence of linear algebra is definitely worth watching,   but it does not cover all the material in the course.</p>"},{"location":"pages/online_resources/#sheldnon-axler-linear-algebra-done-right","title":"Sheldnon Axler: Linear Algebra Done Right","text":"<p>In terms of books, I would recommend \"Linear Algebra Done Right\" by Sheldon Axler. This book takes a different approach to linear algebra, focusing on the concepts and ideas rather than the computational aspects. It is well-written and provides a clear understanding of the subject. The book is available for free online here.</p>"},{"location":"pages/online_resources/#khan-academy","title":"Khan Academy","text":"<p>Khan Acedemy has a comprehensive linear algebra course that covers the basics and more advanced topics. The course includes video lectures, practice exercises, and quizzes to help you reinforce your understanding of the material. The platform is user-friendly and allows you to learn at your own pace. There is also a YouTube playlist that covers the same material.</p>"}]}